<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Explainable AI (XAI) | Fakhruddin Khambaty's Learning Hub</title>
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;500;600;700;800&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body { font-family: 'Nunito', sans-serif; background: linear-gradient(135deg, #fef3c7 0%, #fde68a 50%, #fcd34d 100%); min-height: 100vh; padding: 20px; color: #1e293b; line-height: 2; font-size: 18px; }
        .container { max-width: 900px; margin: 0 auto; }
        .nav { background: rgba(255,255,255,0.95); padding: 15px 30px; border-radius: 15px; margin-bottom: 30px; box-shadow: 0 4px 20px rgba(0,0,0,0.08); display: flex; justify-content: space-between; align-items: center; }
        .nav a { color: #b45309; text-decoration: none; font-weight: 600; display: flex; align-items: center; gap: 8px; }
        .nav a:hover { color: #92400e; }
        .header { text-align: center; padding: 50px 40px; background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%); border-radius: 25px; color: white; margin-bottom: 40px; box-shadow: 0 10px 40px rgba(245, 158, 11, 0.3); }
        .header h1 { font-size: 2.5em; margin-bottom: 15px; font-weight: 800; }
        .header p { font-size: 1.2em; opacity: 0.95; max-width: 700px; margin: 0 auto; }
        .beginner-badge { background: #22c55e; color: white; padding: 8px 20px; border-radius: 25px; font-weight: 700; display: inline-block; margin-bottom: 20px; font-size: 0.9em; }
        .section { background: white; border-radius: 25px; padding: 45px; margin-bottom: 35px; box-shadow: 0 4px 25px rgba(0,0,0,0.08); border: 3px solid #fde68a; }
        .section h2 { color: #d97706; font-size: 1.8em; margin-bottom: 25px; display: flex; align-items: center; gap: 15px; padding-bottom: 15px; border-bottom: 3px solid #fef3c7; }
        .section h3 { color: #b45309; font-size: 1.4em; margin: 35px 0 20px 0; padding-left: 20px; border-left: 5px solid #f59e0b; }
        .section p { font-size: 1.1em; color: #334155; margin-bottom: 20px; }
        .eli5-box { background: linear-gradient(135deg, #dbeafe 0%, #bfdbfe 100%); border-radius: 20px; padding: 30px; margin: 25px 0; border: 3px dashed #3b82f6; }
        .eli5-box h4 { color: #1e40af; font-size: 1.3em; margin-bottom: 15px; }
        .eli5-box p { color: #1e3a8a; font-size: 1.15em; margin-bottom: 10px; }
        .code-block { background: #1e293b; border-radius: 20px; padding: 30px; margin: 25px 0; overflow-x: auto; }
        .code-block pre { margin: 0; font-family: 'Fira Code', monospace; font-size: 0.95em; color: #e2e8f0; line-height: 1.8; }
        .code-block .comment { color: #94a3b8; }
        .code-block .keyword { color: #c084fc; }
        .code-block .function { color: #38bdf8; }
        .code-block .string { color: #4ade80; }
        .download-box { background: linear-gradient(135deg, #dbeafe 0%, #bfdbfe 100%); border-radius: 20px; padding: 25px; margin: 25px 0; border: 3px solid #3b82f6; }
        .nav-buttons { display: flex; justify-content: space-between; margin-top: 50px; gap: 20px; flex-wrap: wrap; }
        .nav-btn { display: inline-flex; align-items: center; gap: 10px; padding: 18px 35px; border-radius: 15px; text-decoration: none; font-weight: 700; transition: all 0.3s; }
        .nav-btn.prev { background: #f1f5f9; color: #475569; }
        .nav-btn.next { background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%); color: white; }
        .nav-btn:hover { transform: translateY(-3px); box-shadow: 0 8px 25px rgba(0,0,0,0.15); }
        .back-to-top { position: fixed; bottom: 30px; right: 30px; width: 55px; height: 55px; background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%); color: white; border: none; border-radius: 50%; cursor: pointer; display: flex; align-items: center; justify-content: center; font-size: 22px; z-index: 1000; opacity: 0; visibility: hidden; transition: all 0.3s; }
        .back-to-top.show { opacity: 1; visibility: visible; }
        @media (max-width: 768px) { body { padding: 10px; font-size: 16px; } .header { padding: 30px 20px; } .section { padding: 25px 20px; } .nav-buttons { flex-direction: column; } }
    </style>
</head>
<body>
    <div class="container">
        <nav class="nav">
            <a href="index.html"><i class="fas fa-arrow-left"></i><span>Back to Course Hub</span></a>
            <a href="index.html"><i class="fas fa-home"></i><span>Course Hub</span></a>
        </nav>

        <div class="header">
            <span class="beginner-badge">üîç TRUST & TRANSPARENCY</span>
            <h1>Explainable AI (XAI)</h1>
            <p>Understand why a model made a prediction: feature importance, SHAP, LIME, and explainer dashboards.</p>
        </div>

        <div class="section">
            <h2><i class="fas fa-question-circle"></i> Why Explainability?</h2>
            <div class="eli5-box">
                <h4>üë∂ In Simple Terms</h4>
                <p>Black-box models (e.g. complex trees, neural nets) give a prediction but not the reason. <strong>Explainable AI (XAI)</strong> answers: ‚ÄúWhich inputs (features) drove this output?‚Äù That builds trust, helps debug, and meets regulatory or fairness needs. Common tools: <strong>feature importance</strong>, <strong>SHAP</strong> (Shapley values), <strong>LIME</strong> (local approximations), and <strong>explainer dashboards</strong> (e.g. Plotly Dash).</p>
            </div>
        </div>

        <div class="section">
            <h2><i class="fas fa-chart-bar"></i> Feature Importance</h2>
            <p>Tree-based models (Random Forest, XGBoost) expose <code>feature_importances_</code>: which features the model used most to split. For linear models, coefficients show how each feature pushes the prediction up or down.</p>
            <div class="code-block">
<pre><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd

<span class="comment"># Train a model (e.g. on diabetes.csv ‚Äì download below)</span>
df = pd.<span class="function">read_csv</span>(<span class="string">"datasets/diabetes.csv"</span>)
X = df.<span class="function">drop</span>(<span class="string">"Outcome"</span>, axis=<span class="number">1</span>)
y = df[<span class="string">"Outcome"</span>]
model = <span class="function">RandomForestClassifier</span>()
model.<span class="function">fit</span>(X, y)

<span class="comment"># Feature importance</span>
<span class="keyword">for</span> name, imp <span class="keyword">in</span> <span class="function">sorted</span>(<span class="function">zip</span>(X.columns, model.feature_importances_), key=<span class="keyword">lambda</span> x: -x[<span class="number">1</span>]):
    <span class="function">print</span>(<span class="string">f"{name}: {imp:.3f}"</span>)</pre>
            </div>
        </div>

        <div class="download-box">
                <strong>üì• Dataset for this lesson:</strong> <a href="datasets/diabetes.csv" download>diabetes.csv</a> ‚Äî save in the same folder as your script or use path <code>datasets/diabetes.csv</code>.
            </div>

        <div class="section">
            <h2><i class="fas fa-balance-scale"></i> SHAP & LIME (Short)</h2>
            <p><strong>SHAP</strong> assigns each feature a contribution (Shapley value) to the prediction for a single sample. <strong>LIME</strong> fits a simple model (e.g. linear) around that sample to approximate the black box locally. Both help answer ‚Äúwhy did the model say 1 for this patient?‚Äù Install: <code>pip install shap lime</code> and use their APIs on your trained model and a single row.</p>
        </div>

        <div class="section">
            <h2><i class="fas fa-tachometer-alt"></i> Explainer Dashboard</h2>
            <p>Tools like <strong>ExplainerDashboard</strong> (Python) or Plotly Dash can wrap your model and SHAP/LIME to build an interactive dashboard: change inputs, see prediction and feature contributions. Great for presenting to stakeholders.</p>
        </div>

            <div class="important-box" style="background: linear-gradient(135deg, #fef2f2 0%, #fecaca 100%); border-radius: 16px; padding: 24px; margin: 25px 0; border-left: 5px solid #ef4444;">
                <h4 style="color: #b91c1c;">üö´ Common Mistakes in Explainable AI</h4>
                <ul style="margin-left: 20px; color: #7f1d1d;">
                    <li><strong>Trusting feature importance without context</strong> ‚Äî High importance can mean correlation, not causation; always sanity-check with domain knowledge.</li>
                    <li><strong>Using LIME/SHAP on the wrong data</strong> ‚Äî Explain on the same preprocessed inputs the model saw; mismatched scaling or encoding gives wrong explanations.</li>
                    <li><strong>Explaining once for all users</strong> ‚Äî Local explanations (per sample) matter for "why this loan was denied"; global importance is for "what drives the model overall."</li>
                </ul>
            </div>

            <div class="reflection-prompt" style="background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%); border-radius: 16px; padding: 24px; margin: 25px 0; border-left: 5px solid #f59e0b;">
                <h4 style="color: #92400e;">üí≠ Short reflection</h4>
                <p style="color: #78350f;">In one sentence: why would a doctor or regulator prefer a model that is explainable (e.g. feature importance, SHAP) over a pure black box?</p>
            </div>
            <div class="core-box" style="background: linear-gradient(135deg, #dcfce7 0%, #bbf7d0 100%); border: 2px solid #22c55e; border-radius: 16px; padding: 24px; margin: 20px 0;">
                <h4 style="color: #166534;">‚úÖ CORE (Must know)</h4>
                <ul style="margin-left: 20px;">
                    <li><strong>Explainable AI (XAI)</strong>: understand why a model made a prediction.</li>
                    <li><strong>Global</strong>: feature importance (e.g. tree-based importances) across the dataset.</li>
                    <li><strong>Local</strong>: SHAP (Shapley values per feature per sample), LIME (local linear approximation).</li>
                    <li>Use for trust, debugging, and compliance.</li>
                </ul>
            </div>
            <div class="noncore-box" style="background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%); border: 2px solid #f59e0b; border-radius: 16px; padding: 24px; margin: 20px 0;">
                <h4 style="color: #92400e;">üìö NON-CORE (Good to know)</h4>
                <ul style="margin-left: 20px;">
                    <li>ExplainerDashboard, counterfactual explanations.</li>
                </ul>
            </div>

        <div class="nav-buttons">
            <a href="index.html" class="nav-btn prev"><i class="fas fa-arrow-left"></i> Back to Course Hub</a>
            <a href="gen-ai-intro.html" class="nav-btn next">Next: Gen AI Intro <i class="fas fa-arrow-right"></i></a>
        </div>
    </div>

    <button class="back-to-top" id="backToTop"><i class="fas fa-arrow-up"></i></button>
    <script>
        const backToTopButton = document.getElementById('backToTop');
        window.addEventListener('scroll', () => {
            if (window.pageYOffset > 300) backToTopButton.classList.add('show');
            else backToTopButton.classList.remove('show');
        });
        backToTopButton.addEventListener('click', () => window.scrollTo({ top: 0, behavior: 'smooth' }));
    </script>
</body>
</html>
