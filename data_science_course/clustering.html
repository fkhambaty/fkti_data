<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Clustering | Fakhruddin Khambaty's Learning Hub</title>
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;500;600;700;800&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Nunito', sans-serif;
            background: linear-gradient(135deg, #e0e7ff 0%, #c7d2fe 50%, #a5b4fc 100%);
            min-height: 100vh;
            padding: 20px;
            color: #1e293b;
            line-height: 1.8;
        }
        .container { max-width: 1000px; margin: 0 auto; }
        
        .nav {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            padding: 15px 30px;
            border-radius: 15px;
            margin-bottom: 30px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.08);
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        .nav a { color: #6366f1; text-decoration: none; font-weight: 600; display: flex; align-items: center; gap: 8px; transition: all 0.3s; }
        .nav a:hover { color: #4338ca; }
        
        .header {
            text-align: center;
            padding: 40px;
            background: linear-gradient(135deg, #6366f1 0%, #4f46e5 100%);
            border-radius: 25px;
            color: white;
            margin-bottom: 40px;
            box-shadow: 0 10px 40px rgba(99, 102, 241, 0.3);
        }
        .header h1 { font-size: 2.8em; margin-bottom: 15px; font-weight: 800; }
        .header p { font-size: 1.3em; opacity: 0.95; max-width: 700px; margin: 0 auto; }
        
        .section {
            background: white;
            border-radius: 20px;
            padding: 40px;
            margin-bottom: 30px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.08);
            border-left: 5px solid #6366f1;
        }
        .section h2 { color: #6366f1; font-size: 2em; margin-bottom: 20px; display: flex; align-items: center; gap: 15px; }
        .section h3 { color: #4f46e5; font-size: 1.5em; margin: 30px 0 15px 0; padding-bottom: 10px; border-bottom: 2px solid #e0e7ff; }
        .section p { font-size: 1.1em; color: #334155; margin-bottom: 15px; }
        
        .analogy-box {
            background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%);
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            border-left: 4px solid #f59e0b;
        }
        .analogy-box h4 { color: #92400e; font-size: 1.2em; margin-bottom: 10px; }
        .analogy-box p { color: #78350f; }
        
        .example-box {
            background: linear-gradient(135deg, #ecfdf5 0%, #d1fae5 100%);
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            border-left: 4px solid #10b981;
        }
        .example-box h4 { color: #065f46; font-size: 1.2em; margin-bottom: 10px; }
        .example-box p, .example-box li { color: #064e3b; }
        
        .key-point {
            background: linear-gradient(135deg, #e0e7ff 0%, #c7d2fe 100%);
            border-radius: 15px;
            padding: 20px;
            margin: 20px 0;
            border-left: 4px solid #6366f1;
        }
        .key-point h4 { color: #4338ca; margin-bottom: 10px; }
        .key-point p { color: #3730a3; }
        
        .code-block {
            background: #1e293b;
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            overflow-x: auto;
        }
        .code-block pre { margin: 0; font-family: 'Fira Code', monospace; font-size: 0.95em; color: #e2e8f0; line-height: 1.6; }
        .code-block .comment { color: #94a3b8; }
        .code-block .keyword { color: #c084fc; }
        .code-block .function { color: #38bdf8; }
        .code-block .string { color: #4ade80; }
        .code-block .number { color: #fb923c; }
        
        .data-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            background: white;
            border-radius: 10px;
            overflow: hidden;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .data-table th {
            background: linear-gradient(135deg, #6366f1 0%, #4f46e5 100%);
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 700;
        }
        .data-table td { padding: 12px 15px; border-bottom: 1px solid #e2e8f0; }
        .data-table tr:nth-child(even) { background: #f0f5ff; }
        
        .cluster-visual {
            background: linear-gradient(135deg, #f0fdf4, #dcfce7);
            border-radius: 15px;
            padding: 30px;
            margin: 25px 0;
            text-align: center;
        }
        .cluster-visual h4 { color: #166534; margin-bottom: 20px; }
        .cluster-dots {
            display: flex;
            justify-content: center;
            gap: 40px;
            flex-wrap: wrap;
            margin-top: 20px;
        }
        .cluster-group {
            padding: 20px 30px;
            border-radius: 50%;
            background: rgba(255,255,255,0.7);
        }
        
        .algo-card {
            background: white;
            border-radius: 15px;
            padding: 25px;
            margin: 20px 0;
            border: 2px solid #e2e8f0;
            transition: all 0.3s;
        }
        .algo-card:hover { transform: translateY(-5px); box-shadow: 0 10px 30px rgba(0,0,0,0.1); }
        .algo-card h4 { color: #6366f1; margin-bottom: 10px; font-size: 1.3em; }
        
        .nav-buttons { display: flex; justify-content: space-between; margin-top: 40px; gap: 20px; flex-wrap: wrap; }
        .nav-btn { display: inline-flex; align-items: center; gap: 10px; padding: 15px 30px; border-radius: 10px; text-decoration: none; font-weight: 600; transition: all 0.3s; }
        .nav-btn.prev { background: #f1f5f9; color: #475569; }
        .nav-btn.next { background: linear-gradient(135deg, #6366f1 0%, #4f46e5 100%); color: white; }
        .nav-btn:hover { transform: translateY(-3px); box-shadow: 0 5px 20px rgba(0,0,0,0.15); }
        
        .back-to-top {
            position: fixed; bottom: 30px; right: 30px; width: 50px; height: 50px;
            background: linear-gradient(135deg, #6366f1 0%, #4f46e5 100%);
            color: white; border: none; border-radius: 50%; cursor: pointer;
            display: flex; align-items: center; justify-content: center;
            font-size: 20px; z-index: 1000; opacity: 0; visibility: hidden; transition: all 0.3s;
        }
        .back-to-top.show { opacity: 1; visibility: visible; }

        /* Graph / chart with clear axes */
        .graph-figure {
            background: #f8fafc;
            border: 2px solid #e2e8f0;
            border-radius: 16px;
            padding: 24px;
            margin: 25px 0;
            text-align: center;
        }
        .graph-figure figcaption {
            font-weight: 600;
            color: #475569;
            margin-top: 12px;
            font-size: 1em;
        }
        .axis-chart {
            max-width: 100%;
            height: auto;
        }

        /* Interactive quiz */
        .interactive-quiz {
            background: linear-gradient(135deg, #e0e7ff 0%, #c7d2fe 100%);
            border-radius: 20px;
            padding: 28px;
            margin: 25px 0;
            border: 2px solid #6366f1;
        }
        .interactive-quiz h4 { color: #4338ca; margin-bottom: 15px; }
        .quiz-option {
            display: block;
            padding: 14px 20px;
            margin: 10px 0;
            background: white;
            border: 2px solid #c7d2fe;
            border-radius: 12px;
            cursor: pointer;
            transition: all 0.2s;
        }
        .quiz-option:hover { border-color: #6366f1; background: #eef2ff; }
        .quiz-option.correct { border-color: #22c55e; background: #dcfce7; }
        .quiz-option.wrong { border-color: #ef4444; background: #fee2e2; }
        .quiz-feedback { margin-top: 15px; padding: 12px; border-radius: 10px; font-weight: 600; display: none; }
        .quiz-feedback.show { display: block; }
        .quiz-feedback.correct-msg { background: #dcfce7; color: #166534; }
        .quiz-feedback.wrong-msg { background: #fee2e2; color: #b91c1c; }

        /* Animated concept box */
        @keyframes pulse-dot {
            0%, 100% { opacity: 1; transform: scale(1); }
            50% { opacity: 0.7; transform: scale(1.1); }
        }
        .animate-dots .cluster-dot { animation: pulse-dot 2s ease-in-out infinite; }

        /* K-Means animation: points "snap" to centroid */
        .kmeans-animation-wrap {
            background: linear-gradient(135deg, #e0e7ff 0%, #c7d2fe 100%);
            border-radius: 20px;
            padding: 30px;
            margin: 25px 0;
            border: 2px solid #6366f1;
            text-align: center;
        }
        .kmeans-animation-wrap h4 { color: #4338ca; margin-bottom: 15px; }
        .kmeans-stage {
            display: inline-block;
            width: 140px;
            margin: 10px;
            vertical-align: top;
        }
        .kmeans-stage .stage-label {
            font-weight: 700;
            color: #4f46e5;
            margin-bottom: 8px;
            font-size: 0.95em;
        }
        .kmeans-arena {
            width: 120px;
            height: 120px;
            margin: 0 auto 8px;
            position: relative;
            background: rgba(255,255,255,0.7);
            border-radius: 12px;
            border: 2px solid #a5b4fc;
        }
        .kmeans-arena .centroid {
            position: absolute;
            width: 16px;
            height: 16px;
            background: #ef4444;
            border: 2px solid #b91c1c;
            border-radius: 50%;
            top: 50%;
            left: 50%;
            margin: -8px 0 0 -8px;
            animation: centroid-pulse 1.5s ease-in-out infinite;
        }
        .kmeans-arena .point {
            position: absolute;
            width: 10px;
            height: 10px;
            background: #6366f1;
            border-radius: 50%;
            animation: point-float 2s ease-in-out infinite;
        }
        .kmeans-arena .point.p1 { top: 15%; left: 20%; animation-delay: 0s; }
        .kmeans-arena .point.p2 { top: 25%; right: 25%; left: auto; animation-delay: 0.2s; }
        .kmeans-arena .point.p3 { bottom: 20%; left: 25%; animation-delay: 0.4s; }
        .kmeans-arena .point.p4 { bottom: 15%; right: 20%; animation-delay: 0.6s; }
        @keyframes centroid-pulse {
            0%, 100% { transform: scale(1); box-shadow: 0 0 0 0 rgba(239,68,68,0.4); }
            50% { transform: scale(1.2); box-shadow: 0 0 0 8px rgba(239,68,68,0); }
        }
        @keyframes point-float {
            0%, 100% { transform: translate(0, 0); }
            25% { transform: translate(2px, -3px); }
            75% { transform: translate(-2px, 2px); }
        }
        .core-box { background: linear-gradient(135deg, #dcfce7 0%, #bbf7d0 100%); border: 2px solid #22c55e; border-radius: 16px; padding: 24px; margin: 20px 0; }
        .core-box h4 { color: #166534; margin-bottom: 12px; }
        .core-box ul, .noncore-box ul { margin-left: 20px; }
        .noncore-box { background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%); border: 2px solid #f59e0b; border-radius: 16px; padding: 24px; margin: 20px 0; }
        .noncore-box h4 { color: #92400e; margin-bottom: 12px; }
        .reflection-prompt { background: linear-gradient(135deg, #e0e7ff 0%, #c7d2fe 100%); border-radius: 16px; padding: 24px; margin: 25px 0; border-left: 5px solid #6366f1; }
        .reflection-prompt h4 { color: #4338ca; margin-bottom: 10px; }
        .reflection-prompt p { color: #3730a3; margin: 0; }
        
        @media (max-width: 768px) {
            body { padding: 10px; }
            .header { padding: 30px 20px; }
            .header h1 { font-size: 2em; }
            .section { padding: 25px 20px; }
            .nav-buttons { flex-direction: column; }
        }
    </style>
</head>
<body>
    <div class="container">
        <nav class="nav">
            <a href="../index.html"><i class="fas fa-home"></i><span>Home</span></a>
            <a href="boosting.html"><i class="fas fa-arrow-left"></i><span>Previous: Boosting</span></a>
            <a href="index.html"><i class="fas fa-th-large"></i><span>Course Hub</span></a>
        </nav>

        <div class="header">
            <h1>üéØ Clustering</h1>
            <p>Group similar items together without labels! Discover hidden patterns in your data with unsupervised learning.</p>
        </div>

        <!-- Part 1: What is Clustering -->
        <div class="section">
            <h2><i class="fas fa-object-group"></i> Part 1: What is Clustering?</h2>
            
            <p>Clustering is <strong>unsupervised learning</strong> - you don't have labels telling you which group each item belongs to. The algorithm discovers groups on its own!</p>

            <h3>üë∂ In One Sentence (Like You're 5)</h3>
            <p><strong>Clustering</strong> means: "Put things that are similar close together, and things that are different far apart‚Äîwithout anyone telling you what the groups are." You only have a list of items (e.g. customers, products); the algorithm finds natural groups by similarity.</p>

            <div class="analogy-box">
                <h4>üçéüçäüçá The Fruit Sorting Analogy</h4>
                <p>Imagine you have a basket of fruits and a child who has never seen fruits before.</p>
                <p style="margin-top: 10px;"><strong>Classification (Supervised):</strong> You tell the child "this is an apple, this is an orange" ‚Üí Child learns to identify new fruits</p>
                <p style="margin-top: 10px;"><strong>Clustering (Unsupervised):</strong> You say "sort these into groups" ‚Üí Child naturally groups by color, size, or shape WITHOUT knowing the names!</p>
            </div>

            <div class="cluster-visual">
                <h4>Clustering Discovers Natural Groups</h4>
                <div class="graph-figure" style="background: rgba(255,255,255,0.8); margin: 20px auto;">
                    <p style="margin-bottom: 8px; font-weight: 600; color: #334155;">Example: 2D data (X and Y are two features). Same lesson in graph form ‚Äì clear X and Y axes.</p>
                    <svg class="axis-chart" viewBox="0 0 380 260" xmlns="http://www.w3.org/2000/svg">
                        <defs><marker id="arr0" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto"><polygon points="0 0, 10 3.5, 0 7" fill="#475569"/></marker></defs>
                        <line x1="50" y1="210" x2="360" y2="210" stroke="#475569" stroke-width="2" marker-end="url(#arr0)"/>
                        <text x="330" y="235" fill="#334155" font-size="13">X (e.g. Feature 1)</text>
                        <line x1="50" y1="210" x2="50" y2="30" stroke="#475569" stroke-width="2" marker-end="url(#arr0)"/>
                        <text x="8" y="45" fill="#334155" font-size="13">Y (e.g. Feature 2)</text>
                        <!-- Cluster A (red) -->
                        <circle cx="100" cy="160" r="6" fill="#ef4444"/>
                        <circle cx="115" cy="150" r="6" fill="#ef4444"/>
                        <circle cx="90" cy="170" r="6" fill="#ef4444"/>
                        <circle cx="105" cy="155" r="6" fill="#ef4444"/>
                        <!-- Cluster B (blue) -->
                        <circle cx="220" cy="80" r="6" fill="#3b82f6"/>
                        <circle cx="235" cy="70" r="6" fill="#3b82f6"/>
                        <circle cx="210" cy="90" r="6" fill="#3b82f6"/>
                        <circle cx="225" cy="85" r="6" fill="#3b82f6"/>
                        <!-- Cluster C (green) -->
                        <circle cx="300" cy="170" r="6" fill="#22c55e"/>
                        <circle cx="315" cy="160" r="6" fill="#22c55e"/>
                        <circle cx="290" cy="180" r="6" fill="#22c55e"/>
                        <circle cx="305" cy="175" r="6" fill="#22c55e"/>
                        <text x="95" y="195" fill="#dc2626" font-size="11" font-weight="600">Cluster A</text>
                        <text x="205" y="115" fill="#2563eb" font-size="11" font-weight="600">Cluster B</text>
                        <text x="285" y="200" fill="#16a34a" font-size="11" font-weight="600">Cluster C</text>
                    </svg>
                    <figcaption>Figure: Scatter plot. X-axis = one feature, Y-axis = another. Colors = 3 clusters found by the algorithm.</figcaption>
                </div>
                <div class="cluster-dots">
                    <div class="cluster-group" style="border: 3px dashed #ef4444;">
                        <span style="font-size: 2em;">üî¥üî¥üî¥</span>
                        <p style="color: #dc2626; font-weight: 600; margin-top: 10px;">Cluster A</p>
                    </div>
                    <div class="cluster-group" style="border: 3px dashed #3b82f6;">
                        <span style="font-size: 2em;">üîµüîµüîµ</span>
                        <p style="color: #2563eb; font-weight: 600; margin-top: 10px;">Cluster B</p>
                    </div>
                    <div class="cluster-group" style="border: 3px dashed #22c55e;">
                        <span style="font-size: 2em;">üü¢üü¢üü¢</span>
                        <p style="color: #16a34a; font-weight: 600; margin-top: 10px;">Cluster C</p>
                    </div>
                </div>
            </div>

            <h3>Real-World Applications</h3>
            <table class="data-table">
                <tr>
                    <th>Industry</th>
                    <th>Clustering Use Case</th>
                </tr>
                <tr>
                    <td>Marketing</td>
                    <td>Customer segmentation (high-value, occasional, bargain hunters)</td>
                </tr>
                <tr>
                    <td>Retail</td>
                    <td>Product categorization, store grouping</td>
                </tr>
                <tr>
                    <td>Healthcare</td>
                    <td>Patient grouping by symptoms, disease subtypes</td>
                </tr>
                <tr>
                    <td>Social Media</td>
                    <td>Community detection, similar user grouping</td>
                </tr>
                <tr>
                    <td>Image Processing</td>
                    <td>Color quantization, image segmentation</td>
                </tr>
            </table>
        </div>

        <!-- Part 2: K-Means Clustering -->
        <div class="section">
            <h2><i class="fas fa-crosshairs"></i> Part 2: K-Means Clustering</h2>
            
            <p>K-Means is the most popular clustering algorithm. It divides data into K clusters by finding K "centers" (centroids).</p>

            <div class="analogy-box">
                <h4>üìç The Pizza Delivery Analogy</h4>
                <p>You need to place K pizza stores to serve a city. Where do you put them?</p>
                <ol style="margin-left: 20px; margin-top: 10px;">
                    <li>Start by randomly placing K stores</li>
                    <li>Each customer goes to the nearest store</li>
                    <li>Move each store to the center of its customer area</li>
                    <li>Repeat until stores stop moving!</li>
                </ol>
                <p style="margin-top: 10px;"><strong>K-Means does exactly this!</strong> "Stores" = Centroids, "Customers" = Data points</p>
            </div>

            <div class="graph-figure" style="background: #f0f4ff;">
                <h4 style="color: #4338ca; margin-bottom: 10px;">üé¨ Animated: One K-Means iteration (Assign ‚Üí Update)</h4>
                <p style="margin-bottom: 10px; font-size: 0.95em;">Left: points (blue) and centroid (red). Right: after assigning points to centroid and moving centroid to their mean. Watch the red circle move.</p>
                <svg viewBox="0 0 380 160" xmlns="http://www.w3.org/2000/svg" style="max-width: 100%;">
                    <g id="before">
                        <circle cx="95" cy="80" r="12" fill="#ef4444" stroke="#b91c1c" stroke-width="2">
                            <animate attributeName="opacity" values="1;0.6;1" dur="2s" repeatCount="indefinite"/>
                        </circle>
                        <circle cx="60" cy="50" r="8" fill="#6366f1"><animate attributeName="cy" values="50;75;50" dur="2.5s" repeatCount="indefinite"/></circle>
                        <circle cx="130" cy="60" r="8" fill="#6366f1"><animate attributeName="cy" values="60;78;60" dur="2.5s" repeatCount="indefinite"/></circle>
                        <circle cx="80" cy="100" r="8" fill="#6366f1"><animate attributeName="cy" values="100;82;100" dur="2.5s" repeatCount="indefinite"/></circle>
                        <text x="95" y="130" fill="#475569" font-size="12">Before update</text>
                    </g>
                    <g id="after">
                        <circle cx="285" cy="78" r="12" fill="#22c55e" stroke="#166534" stroke-width="2">
                            <animate attributeName="opacity" values="0.7;1;0.7" dur="2s" repeatCount="indefinite"/>
                        </circle>
                        <circle cx="260" cy="75" r="8" fill="#6366f1"><animate attributeName="opacity" values="0.8;1;0.8" dur="2s" repeatCount="indefinite"/></circle>
                        <circle cx="310" cy="72" r="8" fill="#6366f1"><animate attributeName="opacity" values="0.8;1;0.8" dur="2s" repeatCount="indefinite"/></circle>
                        <circle cx="280" cy="88" r="8" fill="#6366f1"><animate attributeName="opacity" values="0.8;1;0.8" dur="2s" repeatCount="indefinite"/></circle>
                        <text x="265" y="130" fill="#475569" font-size="12">After update (centroid moved)</text>
                    </g>
                    <path d="M 180 80 L 230 80" stroke="#94a3b8" stroke-width="2" stroke-dasharray="5">
                        <animate attributeName="opacity" values="0.3;1;0.3" dur="1.5s" repeatCount="indefinite"/>
                    </path>
                    <text x="195" y="75" fill="#64748b" font-size="11">‚Üí</text>
                </svg>
                <figcaption>Animation: centroid (red/green) and points. After ‚Äúupdate‚Äù, centroid moves to the mean of its points.</figcaption>
            </div>

            <h3>K-Means Algorithm Steps</h3>
            <div style="display: flex; flex-direction: column; gap: 15px; margin: 20px 0;">
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #6366f1; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">1</div>
                    <div>
                        <h5 style="color: #4f46e5;">Choose K (number of clusters)</h5>
                        <p style="color: #475569; margin: 0;">Decide how many groups you want</p>
                    </div>
                </div>
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #6366f1; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">2</div>
                    <div>
                        <h5 style="color: #4f46e5;">Initialize K random centroids</h5>
                        <p style="color: #475569; margin: 0;">Place K random points as starting cluster centers</p>
                    </div>
                </div>
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #6366f1; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">3</div>
                    <div>
                        <h5 style="color: #4f46e5;">Assign each point to nearest centroid</h5>
                        <p style="color: #475569; margin: 0;">Each data point joins the cluster of its closest centroid</p>
                    </div>
                </div>
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #6366f1; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">4</div>
                    <div>
                        <h5 style="color: #4f46e5;">Update centroids</h5>
                        <p style="color: #475569; margin: 0;">Move each centroid to the average position of its cluster members</p>
                    </div>
                </div>
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #22c55e; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">5</div>
                    <div>
                        <h5 style="color: #166534;">Repeat until convergence</h5>
                        <p style="color: #475569; margin: 0;">Keep assigning and updating until centroids stop moving</p>
                    </div>
                </div>
            </div>

            <div class="code-block">
<pre><span class="comment"># K-Means Clustering in Python</span>
<span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt

<span class="comment"># Step 1: Scale the data (important for K-Means!)</span>
scaler = <span class="function">StandardScaler</span>()
X_scaled = scaler.<span class="function">fit_transform</span>(X)

<span class="comment"># Step 2: Fit K-Means</span>
kmeans = <span class="function">KMeans</span>(n_clusters=<span class="number">3</span>, random_state=<span class="number">42</span>, n_init=<span class="number">10</span>)
kmeans.<span class="function">fit</span>(X_scaled)

<span class="comment"># Step 3: Get cluster labels</span>
clusters = kmeans.labels_
<span class="function">print</span>(<span class="string">f"Cluster assignments: {clusters}"</span>)

<span class="comment"># Step 4: Get cluster centers</span>
centers = kmeans.cluster_centers_
<span class="function">print</span>(<span class="string">f"Cluster centers:\n{centers}"</span>)

<span class="comment"># Step 5: Visualize (for 2D data)</span>
plt.<span class="function">figure</span>(figsize=(<span class="number">10</span>, <span class="number">6</span>))
plt.<span class="function">scatter</span>(X_scaled[:, <span class="number">0</span>], X_scaled[:, <span class="number">1</span>], c=clusters, cmap=<span class="string">'viridis'</span>)
plt.<span class="function">scatter</span>(centers[:, <span class="number">0</span>], centers[:, <span class="number">1</span>], c=<span class="string">'red'</span>, marker=<span class="string">'X'</span>, s=<span class="number">200</span>, label=<span class="string">'Centroids'</span>)
plt.<span class="function">legend</span>()
plt.<span class="function">title</span>(<span class="string">'K-Means Clustering'</span>)
plt.<span class="function">show</span>()</pre>
            </div>

            <h3>üìê Intra-Cluster Distance (Within-Cluster Sum of Squares ‚Äì WCSS)</h3>
            <p><strong>What is it?</strong> For each cluster, we measure how far every point is from its centroid. Intra-cluster distance means ‚Äúdistance <em>inside</em> the cluster.‚Äù K-Means tries to make this as small as possible: points in the same group should be close together.</p>
            <div class="analogy-box">
                <h4>üè† Layman Example: Students in Classrooms</h4>
                <p>Imagine you have 30 students and 3 classrooms (K=3). <strong>Intra-cluster distance</strong> = how spread out the students are <em>within</em> each room. Good clustering = students in the same room are similar (e.g. same grade); they sit close together. Bad clustering = mixed grades in one room, so some sit far from the ‚Äúcenter‚Äù of that room. K-Means keeps moving the ‚Äúcenter‚Äù (centroid) and reassigning students until the total ‚Äúspread‚Äù inside each room is minimized.</p>
            </div>
            <p><strong>Formula (idea):</strong> For each cluster, sum the squared distance of every point to its centroid. Add that up for all K clusters. That total is called <strong>Inertia</strong> or <strong>WCSS</strong>. Lower inertia = tighter, better-separated clusters.</p>
            <div class="graph-figure">
                <p style="margin-bottom: 12px; font-weight: 600; color: #334155;">Concept: Points and their centroid in one cluster (intra-cluster distances)</p>
                <svg class="axis-chart" viewBox="0 0 400 280" xmlns="http://www.w3.org/2000/svg">
                    <defs><marker id="arrowhead" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto"><polygon points="0 0, 10 3.5, 0 7" fill="#64748b"/></marker></defs>
                    <!-- X axis -->
                    <line x1="40" y1="230" x2="380" y2="230" stroke="#475569" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <text x="370" y="255" fill="#334155" font-size="14" font-family="sans-serif">X (Feature 1)</text>
                    <!-- Y axis -->
                    <line x1="40" y1="230" x2="40" y2="30" stroke="#475569" stroke-width="2" marker-end="url(#arrowhead)"/>
                    <text x="5" y="40" fill="#334155" font-size="14" font-family="sans-serif">Y (Feature 2)</text>
                    <!-- Grid -->
                    <line x1="40" y1="180" x2="380" y2="180" stroke="#e2e8f0" stroke-width="1" stroke-dasharray="4"/>
                    <line x1="40" y1="130" x2="380" y2="130" stroke="#e2e8f0" stroke-width="1" stroke-dasharray="4"/>
                    <line x1="40" y1="80" x2="380" y2="80" stroke="#e2e8f0" stroke-width="1" stroke-dasharray="4"/>
                    <line x1="120" y1="30" x2="120" y2="230" stroke="#e2e8f0" stroke-width="1" stroke-dasharray="4"/>
                    <line x1="200" y1="30" x2="200" y2="230" stroke="#e2e8f0" stroke-width="1" stroke-dasharray="4"/>
                    <line x1="280" y1="30" x2="280" y2="230" stroke="#e2e8f0" stroke-width="1" stroke-dasharray="4"/>
                    <!-- Cluster points (same cluster) -->
                    <circle cx="140" cy="160" r="8" fill="#6366f1" opacity="0.9"/>
                    <circle cx="165" cy="145" r="8" fill="#6366f1" opacity="0.9"/>
                    <circle cx="155" cy="175" r="8" fill="#6366f1" opacity="0.9"/>
                    <circle cx="180" cy="165" r="8" fill="#6366f1" opacity="0.9"/>
                    <circle cx="160" cy="155" r="8" fill="#6366f1" opacity="0.9"/>
                    <!-- Centroid -->
                    <circle cx="160" cy="160" r="12" fill="#ef4444" stroke="#b91c1c" stroke-width="2"/>
                    <text x="158" y="165" fill="white" font-size="10" font-weight="bold" text-anchor="middle">C</text>
                    <!-- Distance lines (intra-cluster) -->
                    <line x1="160" y1="160" x2="140" y2="160" stroke="#f59e0b" stroke-width="1.5" stroke-dasharray="3"/>
                    <line x1="160" y1="160" x2="180" y2="165" stroke="#f59e0b" stroke-width="1.5" stroke-dasharray="3"/>
                    <text x="200" y="50" fill="#64748b" font-size="12">C = centroid; dashed = intra-cluster distances</text>
                </svg>
                <figcaption>Figure: One cluster. Red = centroid (C). Purple = data points. Orange dashed = distances counted in WCSS.</figcaption>
            </div>

            <h3>üìê Inter-Cluster Distance (Between Clusters)</h3>
            <p><strong>What is it?</strong> While intra-cluster distance measures how tight each group is, <strong>inter-cluster distance</strong> measures how far apart different clusters are from each other. A good clustering has <strong>small intra</strong> (points close to their centroid) and <strong>large inter</strong> (centroids far from each other).</p>
            <div class="analogy-box">
                <h4>üè† Layman Example: Classrooms Again</h4>
                <p><strong>Intra</strong> = students inside one room are close together. <strong>Inter</strong> = Room A and Room B are in different corridors. We want rooms that are clearly separated (high inter) and students in each room sitting near each other (low intra).</p>
            </div>
            <p>In K-Means we minimize WCSS (intra). We don‚Äôt directly maximize inter-cluster distance, but when clusters are well separated, both happen: tight groups and far-apart centroids.</p>

            <h3>üîÑ Convergence and Random Initialization</h3>
            <p><strong>Convergence</strong> means the algorithm stops when centroids barely move between steps. We repeat ‚Äúassign points ‚Üí update centroids‚Äù until the change in centroid positions is below a threshold (or max iterations).</p>
            <p><strong>Random initialization</strong> can give different results each run. <strong>K-Means++</strong> is a smarter way to choose initial centroids: pick the first at random, then choose the next ones with probability proportional to how far they are from already chosen centroids. This usually leads to better and more stable clusters.</p>

            <h3>üìè Distance Metrics (Euclidean vs Manhattan)</h3>
            <p>K-Means uses <strong>Euclidean distance</strong> by default (straight-line ‚Äúas the crow flies‚Äù). For high-dimensional or grid-like data, <strong>Manhattan distance</strong> (sum of absolute differences along axes) is sometimes used; in scikit-learn you can use <code>KMeans(metric='manhattan')</code> with the K-Medians idea. For most tabular data, Euclidean is standard.</p>

            <div class="kmeans-animation-wrap">
                <h4>üé¨ K-Means in Action (Animated)</h4>
                <p style="margin-bottom: 15px; color: #475569;">Watch: red = centroid (pulses); blue = data points (slight float). In real K-Means, points ‚Äúsnap‚Äù to the nearest centroid, then the centroid moves to the center of its points. Repeat until stable.</p>
                <div class="kmeans-stage">
                    <div class="stage-label">Step 1: Assign</div>
                    <div class="kmeans-arena">
                        <div class="centroid"></div>
                        <div class="point p1"></div>
                        <div class="point p2"></div>
                        <div class="point p3"></div>
                        <div class="point p4"></div>
                    </div>
                    <p style="font-size: 0.85em; color: #64748b;">Points ‚Üí nearest centroid</p>
                </div>
                <div class="kmeans-stage">
                    <div class="stage-label">Step 2: Update</div>
                    <div class="kmeans-arena">
                        <div class="centroid"></div>
                        <div class="point p1"></div>
                        <div class="point p2"></div>
                        <div class="point p3"></div>
                        <div class="point p4"></div>
                    </div>
                    <p style="font-size: 0.85em; color: #64748b;">Centroid ‚Üí mean of points</p>
                </div>
                <div class="kmeans-stage">
                    <div class="stage-label">Repeat until done</div>
                    <div class="kmeans-arena">
                        <div class="centroid"></div>
                        <div class="point p1"></div>
                        <div class="point p2"></div>
                        <div class="point p3"></div>
                        <div class="point p4"></div>
                    </div>
                    <p style="font-size: 0.85em; color: #64748b;">Convergence</p>
                </div>
            </div>

            <h3>‚ö†Ô∏è Dealing with Outliers in K-Means</h3>
            <p>K-Means is <strong>sensitive to outliers</strong>. One point far away can pull a centroid toward it and distort the whole cluster. Here‚Äôs how to deal with it:</p>
            <ul style="margin-left: 24px; margin-bottom: 15px;">
                <li><strong>Detect outliers first:</strong> Use boxplots, IQR, or Z-scores (see the Missing Values &amp; Outliers lesson).</li>
                <li><strong>Remove them:</strong> Drop extreme points before running K-Means if they are errors or irrelevant.</li>
                <li><strong>Cap/winsorize:</strong> Replace extreme values with a max/min threshold so they don‚Äôt dominate.</li>
                <li><strong>Use robust scaling:</strong> Scale with RobustScaler (median and IQR) instead of StandardScaler so one extreme value doesn‚Äôt stretch the scale.</li>
                <li><strong>Try K-Medians:</strong> Use the median instead of the mean when updating centroids; medians are less pulled by outliers.</li>
                <li><strong>Use DBSCAN:</strong> If you have many outliers or odd shapes, DBSCAN labels them as ‚Äúnoise‚Äù instead of forcing them into a cluster.</li>
            </ul>
            <div class="key-point">
                <h4>üí° Takeaway</h4>
                <p>Always check for outliers before K-Means. Either clean them, robust-scale, or switch to an algorithm that handles outliers (e.g. DBSCAN).</p>
            </div>
        </div>

        <!-- Part 3: Choosing K -->
        <div class="section">
            <h2><i class="fas fa-question-circle"></i> Part 3: Choosing the Right K</h2>
            
            <p>The biggest challenge in K-Means: <strong>How many clusters should you use?</strong></p>

            <h3>Method 1: Elbow Method</h3>
            <p>Plot the "inertia" (within-cluster sum of squares = WCSS) for different K values. Look for the <strong>elbow</strong> ‚Äì where the curve bends and adding more clusters gives diminishing returns.</p>
            <div class="graph-figure">
                <p style="margin-bottom: 8px; font-weight: 600; color: #334155;">Elbow method: X-axis = Number of clusters (K), Y-axis = Inertia (WCSS)</p>
                <svg class="axis-chart" viewBox="0 0 420 300" xmlns="http://www.w3.org/2000/svg">
                    <defs><marker id="arr2" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto"><polygon points="0 0, 10 3.5, 0 7" fill="#475569"/></marker></defs>
                    <!-- axes -->
                    <line x1="50" y1="250" x2="400" y2="250" stroke="#475569" stroke-width="2" marker-end="url(#arr2)"/>
                    <text x="380" y="275" fill="#334155" font-size="13" font-family="sans-serif">Number of clusters (K)</text>
                    <line x1="50" y1="250" x2="50" y2="40" stroke="#475569" stroke-width="2" marker-end="url(#arr2)"/>
                    <text x="12" y="55" fill="#334155" font-size="13" font-family="sans-serif">Inertia (WCSS)</text>
                    <!-- Y ticks -->
                    <line x1="45" y1="250" x2="50" y2="250" stroke="#475569"/><text x="30" y="255" fill="#475569" font-size="11">0</text>
                    <line x1="45" y1="180" x2="50" y2="180" stroke="#475569"/><text x="25" y="184" fill="#475569" font-size="11">200</text>
                    <line x1="45" y1="110" x2="50" y2="110" stroke="#475569"/><text x="25" y="114" fill="#475569" font-size="11">400</text>
                    <line x1="45" y1="40" x2="50" y2="40" stroke="#475569"/><text x="25" y="44" fill="#475569" font-size="11">600</text>
                    <!-- X ticks -->
                    <line x1="50" y1="250" x2="50" y2="255" stroke="#475569"/><text x="45" y="270" fill="#475569" font-size="11">1</text>
                    <line x1="120" y1="250" x2="120" y2="255" stroke="#475569"/><text x="108" y="270" fill="#475569" font-size="11">3</text>
                    <line x1="190" y1="250" x2="190" y2="255" stroke="#475569"/><text x="178" y="270" fill="#475569" font-size="11">5</text>
                    <line x1="260" y1="250" x2="260" y2="255" stroke="#475569"/><text x="248" y="270" fill="#475569" font-size="11">7</text>
                    <line x1="330" y1="250" x2="330" y2="255" stroke="#475569"/><text x="318" y="270" fill="#475569" font-size="11">9</text>
                    <line x1="400" y1="250" x2="400" y2="255" stroke="#475569"/><text x="388" y="270" fill="#475569" font-size="11">10</text>
                    <!-- Elbow curve (simplified) -->
                    <polyline points="50,55 120,95 190,155 260,195 330,220 400,230" fill="none" stroke="#6366f1" stroke-width="3" stroke-linecap="round" stroke-linejoin="round"/>
                    <circle cx="190" cy="155" r="10" fill="#ef4444" stroke="#b91c1c" stroke-width="2"/>
                    <text x="190" y="140" fill="#b91c1c" font-size="12" font-weight="bold" text-anchor="middle">Elbow (K‚âà5)</text>
                </svg>
                <figcaption>Figure: As K increases, inertia drops. The ‚Äúelbow‚Äù (red dot) suggests a good K ‚Äì here around 5.</figcaption>
            </div>

            <div class="code-block">
<pre><span class="comment"># Elbow Method</span>
inertias = []
K_range = <span class="function">range</span>(<span class="number">1</span>, <span class="number">11</span>)

<span class="keyword">for</span> k <span class="keyword">in</span> K_range:
    kmeans = <span class="function">KMeans</span>(n_clusters=k, random_state=<span class="number">42</span>, n_init=<span class="number">10</span>)
    kmeans.<span class="function">fit</span>(X_scaled)
    inertias.<span class="function">append</span>(kmeans.inertia_)

<span class="comment"># Plot</span>
plt.<span class="function">figure</span>(figsize=(<span class="number">10</span>, <span class="number">6</span>))
plt.<span class="function">plot</span>(K_range, inertias, <span class="string">'bo-'</span>)
plt.<span class="function">xlabel</span>(<span class="string">'Number of Clusters (K)'</span>)
plt.<span class="function">ylabel</span>(<span class="string">'Inertia'</span>)
plt.<span class="function">title</span>(<span class="string">'Elbow Method - Finding Optimal K'</span>)
plt.<span class="function">show</span>()

<span class="comment"># Look for the "elbow" - where the curve bends</span></pre>
            </div>

            <h3>Method 2: Silhouette Score</h3>
            <p>Measures how similar points are to their own cluster vs other clusters. Higher = better!</p>

            <div class="code-block">
<pre><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score

silhouette_scores = []

<span class="keyword">for</span> k <span class="keyword">in</span> <span class="function">range</span>(<span class="number">2</span>, <span class="number">11</span>):  <span class="comment"># Start from 2 (need at least 2 clusters)</span>
    kmeans = <span class="function">KMeans</span>(n_clusters=k, random_state=<span class="number">42</span>, n_init=<span class="number">10</span>)
    labels = kmeans.<span class="function">fit_predict</span>(X_scaled)
    score = <span class="function">silhouette_score</span>(X_scaled, labels)
    silhouette_scores.<span class="function">append</span>(score)
    <span class="function">print</span>(<span class="string">f"K={k}: Silhouette Score = {score:.3f}"</span>)

<span class="comment"># Plot</span>
plt.<span class="function">figure</span>(figsize=(<span class="number">10</span>, <span class="number">6</span>))
plt.<span class="function">plot</span>(<span class="function">range</span>(<span class="number">2</span>, <span class="number">11</span>), silhouette_scores, <span class="string">'go-'</span>)
plt.<span class="function">xlabel</span>(<span class="string">'Number of Clusters (K)'</span>)
plt.<span class="function">ylabel</span>(<span class="string">'Silhouette Score'</span>)
plt.<span class="function">title</span>(<span class="string">'Silhouette Method - Finding Optimal K'</span>)
plt.<span class="function">show</span>()

<span class="comment"># Choose K with highest silhouette score</span></pre>
            </div>

            <div class="key-point">
                <h4>üí° Silhouette Score Interpretation</h4>
                <ul style="margin-left: 20px; color: #3730a3;">
                    <li><strong>+1:</strong> Perfect! Points are very well matched to their cluster</li>
                    <li><strong>0:</strong> Points are on the boundary between clusters</li>
                    <li><strong>-1:</strong> Points might be in the wrong cluster</li>
                </ul>
            </div>
        </div>

        <!-- Part 4: Hierarchical Clustering (Deep Dive) -->
        <div class="section">
            <h2><i class="fas fa-sitemap"></i> Part 4: Hierarchical Clustering</h2>

            <h3>üë∂ The Problem in Plain English</h3>
            <p>With K-Means, you have to pick the number of groups (K) <em>before</em> running the algorithm. But what if you don't know how many groups there are? What if you want to <strong>see all possible groupings</strong> ‚Äî from "every item is its own group" down to "everything is one big group" ‚Äî and then <em>pick</em> the best level?</p>
            <p>That's what <strong>Hierarchical Clustering</strong> does. It builds a <strong>tree of merges</strong> (called a <em>dendrogram</em>) that shows you exactly how items combined, step by step. You look at the tree and decide where to "cut" it.</p>

            <div class="analogy-box">
                <h4>üè´ The School Analogy</h4>
                <p>Imagine a school with 30 students. At first, each student is their own "group of 1."</p>
                <ol style="margin-left: 20px; margin-top: 10px;">
                    <li>The two most similar students (say, best friends Alice and Bob) merge into a group of 2.</li>
                    <li>Next, the two closest things merge ‚Äî maybe Carol joins Alice+Bob, or two other students merge.</li>
                    <li>This keeps going: small groups merge into bigger groups.</li>
                    <li>Eventually, everyone is in one giant group.</li>
                </ol>
                <p style="margin-top: 10px;">The <strong>dendrogram</strong> is like a family tree that records every merge. You can "cut" the tree at any height to get 2, 3, 4, or any number of groups.</p>
            </div>

            <h3>How It Works ‚Äî Step by Step</h3>
            <div style="display: flex; flex-direction: column; gap: 15px; margin: 20px 0;">
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #6366f1; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">1</div>
                    <div><h5 style="color: #4f46e5;">Start: every point is its own cluster</h5><p style="color: #475569; margin: 0;">If you have 100 data points, you start with 100 clusters (each containing 1 point).</p></div>
                </div>
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #6366f1; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">2</div>
                    <div><h5 style="color: #4f46e5;">Find the two closest clusters</h5><p style="color: #475569; margin: 0;">Measure distance between every pair. The closest pair merges into one cluster. Now you have 99 clusters.</p></div>
                </div>
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #6366f1; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">3</div>
                    <div><h5 style="color: #4f46e5;">Repeat</h5><p style="color: #475569; margin: 0;">Keep merging the two closest clusters. 99 ‚Üí 98 ‚Üí 97 ‚Üí ‚Ä¶ ‚Üí 1. Every merge is recorded.</p></div>
                </div>
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #22c55e; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">4</div>
                    <div><h5 style="color: #166534;">Read the dendrogram and cut</h5><p style="color: #475569; margin: 0;">The tree shows all merges. You "cut" at a height that gives you the number of clusters you want. A big vertical gap in the dendrogram = a natural place to cut.</p></div>
                </div>
            </div>

            <h3>What Is a Dendrogram?</h3>
            <p>A dendrogram is a tree diagram. The <strong>X-axis</strong> has the data points (or their indices). The <strong>Y-axis</strong> is the distance at which clusters merge. The higher two branches connect, the more different those clusters are.</p>

            <div class="graph-figure">
                <h4 style="color: #4338ca; margin-bottom: 10px;">Dendrogram ‚Äî How to Read It</h4>
                <svg class="axis-chart" viewBox="0 0 420 260" xmlns="http://www.w3.org/2000/svg">
                    <defs><marker id="arrDend" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto"><polygon points="0 0, 10 3.5, 0 7" fill="#475569"/></marker></defs>
                    <!-- Y axis -->
                    <line x1="50" y1="230" x2="50" y2="20" stroke="#475569" stroke-width="2" marker-end="url(#arrDend)"/>
                    <text x="8" y="35" fill="#334155" font-size="12" font-weight="600">Distance</text>
                    <!-- X axis -->
                    <line x1="50" y1="230" x2="400" y2="230" stroke="#475569" stroke-width="2"/>
                    <text x="180" y="255" fill="#334155" font-size="12">Data Points</text>
                    <!-- Y ticks -->
                    <text x="35" y="230" fill="#64748b" font-size="10" text-anchor="end">0</text>
                    <text x="35" y="180" fill="#64748b" font-size="10" text-anchor="end">2</text>
                    <text x="35" y="130" fill="#64748b" font-size="10" text-anchor="end">4</text>
                    <text x="35" y="80" fill="#64748b" font-size="10" text-anchor="end">6</text>
                    <!-- Points on X axis -->
                    <text x="100" y="245" fill="#6366f1" font-size="11" font-weight="600" text-anchor="middle">A</text>
                    <text x="150" y="245" fill="#6366f1" font-size="11" font-weight="600" text-anchor="middle">B</text>
                    <text x="220" y="245" fill="#22c55e" font-size="11" font-weight="600" text-anchor="middle">C</text>
                    <text x="280" y="245" fill="#22c55e" font-size="11" font-weight="600" text-anchor="middle">D</text>
                    <text x="350" y="245" fill="#ef4444" font-size="11" font-weight="600" text-anchor="middle">E</text>
                    <!-- Merge A+B at distance 1 -->
                    <line x1="100" y1="230" x2="100" y2="205" stroke="#6366f1" stroke-width="2"/>
                    <line x1="150" y1="230" x2="150" y2="205" stroke="#6366f1" stroke-width="2"/>
                    <line x1="100" y1="205" x2="150" y2="205" stroke="#6366f1" stroke-width="2"/>
                    <!-- Merge C+D at distance 1.5 -->
                    <line x1="220" y1="230" x2="220" y2="193" stroke="#22c55e" stroke-width="2"/>
                    <line x1="280" y1="230" x2="280" y2="193" stroke="#22c55e" stroke-width="2"/>
                    <line x1="220" y1="193" x2="280" y2="193" stroke="#22c55e" stroke-width="2"/>
                    <!-- Merge AB + CD at distance 4 -->
                    <line x1="125" y1="205" x2="125" y2="130" stroke="#8b5cf6" stroke-width="2"/>
                    <line x1="250" y1="193" x2="250" y2="130" stroke="#8b5cf6" stroke-width="2"/>
                    <line x1="125" y1="130" x2="250" y2="130" stroke="#8b5cf6" stroke-width="2"/>
                    <!-- Merge ABCD + E at distance 7 -->
                    <line x1="187" y1="130" x2="187" y2="60" stroke="#ef4444" stroke-width="2"/>
                    <line x1="350" y1="230" x2="350" y2="60" stroke="#ef4444" stroke-width="2"/>
                    <line x1="187" y1="60" x2="350" y2="60" stroke="#ef4444" stroke-width="2"/>
                    <!-- Cut line -->
                    <line x1="45" y1="155" x2="400" y2="155" stroke="#f59e0b" stroke-width="2" stroke-dasharray="8,4"/>
                    <text x="370" y="150" fill="#f59e0b" font-size="11" font-weight="700">‚úÇÔ∏è Cut here ‚Üí 3 clusters</text>
                </svg>
                <figcaption>Dendrogram: A &amp; B merge first (closest). C &amp; D merge next. Then AB joins CD. Finally E joins all. Cut the dashed line to get 3 clusters: {A,B}, {C,D}, {E}.</figcaption>
            </div>

            <div class="example-box">
                <h4>üõí Real-World Example: Customer Segmentation</h4>
                <p><strong>Problem:</strong> An online store has 500 customers. You want to group them by spending behavior, but you don't know how many segments exist. Should it be 2? 3? 5?</p>
                <p style="margin-top: 10px;"><strong>Solution:</strong> Run hierarchical clustering. Look at the dendrogram. You see a big gap between 3 and 4 clusters ‚Äî so you cut at 3. The 3 groups turn out to be: "Big Spenders," "Occasional Buyers," and "Window Shoppers." Now marketing can tailor campaigns for each group!</p>
            </div>

            <h3>Linkage Methods ‚Äî How to Measure "Distance Between Clusters"</h3>
            <p>When two clusters have multiple points, how do you measure the distance between them? There are several strategies:</p>
            <table class="data-table">
                <tr><th>Method</th><th>How It Measures Distance</th><th>When to Use</th></tr>
                <tr><td><strong>Ward</strong> (most common)</td><td>Minimizes the total variance within clusters when merging</td><td>Default choice; produces compact, evenly sized clusters</td></tr>
                <tr><td><strong>Single</strong></td><td>Distance between the two <em>closest</em> points of each cluster</td><td>Can find chain-like, elongated clusters</td></tr>
                <tr><td><strong>Complete</strong></td><td>Distance between the two <em>farthest</em> points of each cluster</td><td>Produces compact clusters; sensitive to outliers</td></tr>
                <tr><td><strong>Average</strong></td><td>Average distance between all pairs of points</td><td>Compromise between single and complete</td></tr>
            </table>

            <h3>Python Code ‚Äî Full Example</h3>
            <div class="code-block">
<pre><span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt
<span class="keyword">from</span> scipy.cluster.hierarchy <span class="keyword">import</span> dendrogram, linkage, fcluster
<span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> AgglomerativeClustering
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler

<span class="comment"># --- Step 1: Prepare data (e.g. customer data) ---</span>
<span class="comment"># Columns: Annual Spending ($), Visit Frequency (per month)</span>
X = np.array([[120, 15], [130, 14], [22, 3], [28, 4],
              [300, 25], [280, 22], [25, 2], [135, 16]])

scaler = <span class="function">StandardScaler</span>()
X_scaled = scaler.<span class="function">fit_transform</span>(X)

<span class="comment"># --- Step 2: Build the dendrogram ---</span>
linkage_matrix = <span class="function">linkage</span>(X_scaled, method=<span class="string">'ward'</span>)

plt.<span class="function">figure</span>(figsize=(<span class="number">10</span>, <span class="number">5</span>))
<span class="function">dendrogram</span>(linkage_matrix, labels=[<span class="string">'C1'</span>,<span class="string">'C2'</span>,<span class="string">'C3'</span>,<span class="string">'C4'</span>,<span class="string">'C5'</span>,<span class="string">'C6'</span>,<span class="string">'C7'</span>,<span class="string">'C8'</span>])
plt.<span class="function">title</span>(<span class="string">'Customer Dendrogram'</span>)
plt.<span class="function">xlabel</span>(<span class="string">'Customer'</span>)
plt.<span class="function">ylabel</span>(<span class="string">'Distance (Ward)'</span>)
plt.<span class="function">axhline</span>(y=<span class="number">4</span>, color=<span class="string">'red'</span>, linestyle=<span class="string">'--'</span>, label=<span class="string">'Cut at 3 clusters'</span>)
plt.<span class="function">legend</span>()
plt.<span class="function">show</span>()

<span class="comment"># --- Step 3: Cut the dendrogram to get cluster labels ---</span>
labels_from_dendro = <span class="function">fcluster</span>(linkage_matrix, t=<span class="number">3</span>, criterion=<span class="string">'maxclust'</span>)
<span class="function">print</span>(<span class="string">"Cluster labels:"</span>, labels_from_dendro)

<span class="comment"># --- OR: Use sklearn (same result, easier for pipelines) ---</span>
model = <span class="function">AgglomerativeClustering</span>(n_clusters=<span class="number">3</span>, linkage=<span class="string">'ward'</span>)
labels_sklearn = model.<span class="function">fit_predict</span>(X_scaled)
<span class="function">print</span>(<span class="string">"Sklearn labels:"</span>, labels_sklearn)</pre>
            </div>

            <div class="key-point">
                <h4>ü§î When to Use Hierarchical Clustering Instead of K-Means</h4>
                <ul style="margin-left: 20px; color: #3730a3;">
                    <li>You <strong>don't know K</strong> and want to explore different numbers of clusters visually.</li>
                    <li>You want to see the <strong>hierarchy</strong> ‚Äî which items are most similar, which merge early vs late.</li>
                    <li>Your dataset is <strong>small to medium</strong> (under ~10,000 points). For large data, K-Means is faster.</li>
                    <li>You need <strong>non-spherical clusters</strong> (single/complete linkage can find chains or bands).</li>
                </ul>
            </div>
        </div>

        <!-- Part 5: DBSCAN (Deep Dive) -->
        <div class="section">
            <h2><i class="fas fa-dot-circle"></i> Part 5: DBSCAN ‚Äî Density-Based Clustering</h2>

            <h3>üë∂ The Problem in Plain English</h3>
            <p>K-Means finds <strong>round blobs</strong>. But what if your data has <strong>weird shapes</strong>? Like a crescent moon and a circle, or clusters with <strong>outliers</strong> (noise points that don't belong anywhere)? K-Means will force every point into a cluster, even the outliers. DBSCAN solves both problems: it finds clusters of <em>any shape</em> and says "this point is noise" for outliers.</p>

            <div class="analogy-box">
                <h4>üèôÔ∏è The City Neighborhoods Analogy</h4>
                <p>Imagine looking at a city from above at night. You see <strong>dense clusters of lights</strong> ‚Äî those are neighborhoods. Between them, there are <strong>dark, empty areas</strong>. And a few <strong>lone houses</strong> in the middle of nowhere.</p>
                <ul style="margin-left: 20px; margin-top: 10px;">
                    <li><strong>Dense areas (many lights close together)</strong> = Clusters</li>
                    <li><strong>Dark gaps</strong> = Cluster boundaries</li>
                    <li><strong>Lone houses far from everything</strong> = Outliers / Noise</li>
                </ul>
                <p style="margin-top: 10px;">DBSCAN does exactly this: it finds dense regions of points and calls sparse points "noise."</p>
            </div>

            <h3>Two Parameters You Need to Know</h3>
            <table class="data-table">
                <tr><th>Parameter</th><th>What It Means</th><th>Analogy</th></tr>
                <tr><td><strong>eps</strong> (epsilon)</td><td>The radius of the neighborhood around each point. "How close do two points need to be to be considered neighbors?"</td><td>If you shine a flashlight with radius <code>eps</code> from any point, which other points are lit up?</td></tr>
                <tr><td><strong>min_samples</strong></td><td>The minimum number of points within <code>eps</code> distance to form a dense region (a "core point").</td><td>A neighborhood needs at least <code>min_samples</code> houses within flashlight range to count as a real neighborhood.</td></tr>
            </table>

            <h3>Three Types of Points</h3>
            <div style="display: flex; gap: 15px; flex-wrap: wrap; margin: 20px 0;">
                <div style="flex: 1; min-width: 200px; background: #dcfce7; border-radius: 15px; padding: 20px; border: 2px solid #22c55e; text-align: center;">
                    <div style="font-size: 2em;">üü¢</div>
                    <h5 style="color: #166534;">Core Point</h5>
                    <p style="color: #064e3b; font-size: 0.95em;">Has at least <code>min_samples</code> neighbors within <code>eps</code>. It is in the heart of a cluster.</p>
                </div>
                <div style="flex: 1; min-width: 200px; background: #fef3c7; border-radius: 15px; padding: 20px; border: 2px solid #f59e0b; text-align: center;">
                    <div style="font-size: 2em;">üü°</div>
                    <h5 style="color: #92400e;">Border Point</h5>
                    <p style="color: #78350f; font-size: 0.95em;">Within <code>eps</code> of a core point, but doesn't have enough neighbors to be a core itself. It's on the edge.</p>
                </div>
                <div style="flex: 1; min-width: 200px; background: #fee2e2; border-radius: 15px; padding: 20px; border: 2px solid #ef4444; text-align: center;">
                    <div style="font-size: 2em;">üî¥</div>
                    <h5 style="color: #b91c1c;">Noise Point</h5>
                    <p style="color: #991b1b; font-size: 0.95em;">Not within <code>eps</code> of any core point. It's an outlier ‚Äî doesn't belong to any cluster. Labeled <code>-1</code>.</p>
                </div>
            </div>

            <h3>How DBSCAN Works ‚Äî Step by Step</h3>
            <div style="display: flex; flex-direction: column; gap: 15px; margin: 20px 0;">
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #6366f1; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">1</div>
                    <div><h5 style="color: #4f46e5;">Pick any unvisited point</h5><p style="color: #475569; margin: 0;">Start with a random point.</p></div>
                </div>
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #6366f1; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">2</div>
                    <div><h5 style="color: #4f46e5;">Check its neighborhood</h5><p style="color: #475569; margin: 0;">Count how many points are within <code>eps</code> distance. If count &ge; <code>min_samples</code> ‚Üí it's a <strong>core point</strong>. Start a new cluster!</p></div>
                </div>
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #6366f1; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">3</div>
                    <div><h5 style="color: #4f46e5;">Expand the cluster</h5><p style="color: #475569; margin: 0;">Add all neighbors to this cluster. Check <em>their</em> neighbors too ‚Äî if they are also core points, keep expanding. The cluster grows like a chain through dense regions.</p></div>
                </div>
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #6366f1; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">4</div>
                    <div><h5 style="color: #4f46e5;">Move to the next unvisited point</h5><p style="color: #475569; margin: 0;">If it's a core point ‚Üí start another cluster. If not enough neighbors ‚Üí mark it as <strong>noise (-1)</strong>.</p></div>
                </div>
                <div style="display: flex; align-items: flex-start; gap: 20px; background: #f8fafc; padding: 20px; border-radius: 15px;">
                    <div style="width: 40px; height: 40px; background: #22c55e; border-radius: 50%; display: flex; align-items: center; justify-content: center; font-weight: 700; color: white; flex-shrink: 0;">5</div>
                    <div><h5 style="color: #166534;">Done when all points are visited</h5><p style="color: #475569; margin: 0;">Every point is either in a cluster or labeled noise.</p></div>
                </div>
            </div>

            <div class="graph-figure">
                <h4 style="color: #4338ca; margin-bottom: 10px;">DBSCAN Visualization ‚Äî Core, Border, and Noise Points</h4>
                <svg class="axis-chart" viewBox="0 0 400 260" xmlns="http://www.w3.org/2000/svg">
                    <defs><marker id="arrDB" markerWidth="10" markerHeight="7" refX="9" refY="3.5" orient="auto"><polygon points="0 0, 10 3.5, 0 7" fill="#475569"/></marker></defs>
                    <line x1="40" y1="220" x2="380" y2="220" stroke="#475569" stroke-width="2" marker-end="url(#arrDB)"/>
                    <line x1="40" y1="220" x2="40" y2="20" stroke="#475569" stroke-width="2" marker-end="url(#arrDB)"/>
                    <!-- Cluster 1 (dense) -->
                    <circle cx="110" cy="150" r="40" fill="none" stroke="#6366f1" stroke-width="1" stroke-dasharray="4"/>
                    <circle cx="100" cy="140" r="7" fill="#22c55e" stroke="#166534" stroke-width="2"/><title>Core</title>
                    <circle cx="115" cy="155" r="7" fill="#22c55e" stroke="#166534" stroke-width="2"/>
                    <circle cx="95" cy="160" r="7" fill="#22c55e" stroke="#166534" stroke-width="2"/>
                    <circle cx="120" cy="140" r="7" fill="#22c55e" stroke="#166534" stroke-width="2"/>
                    <circle cx="130" cy="165" r="5" fill="#f59e0b" stroke="#92400e" stroke-width="1.5"/><title>Border</title>
                    <!-- Cluster 2 (dense) -->
                    <circle cx="280" cy="80" r="38" fill="none" stroke="#6366f1" stroke-width="1" stroke-dasharray="4"/>
                    <circle cx="270" cy="75" r="7" fill="#22c55e" stroke="#166534" stroke-width="2"/>
                    <circle cx="290" cy="85" r="7" fill="#22c55e" stroke="#166534" stroke-width="2"/>
                    <circle cx="280" cy="70" r="7" fill="#22c55e" stroke="#166534" stroke-width="2"/>
                    <circle cx="295" cy="68" r="5" fill="#f59e0b" stroke="#92400e" stroke-width="1.5"/>
                    <!-- Noise -->
                    <circle cx="200" cy="190" r="6" fill="#ef4444" stroke="#b91c1c" stroke-width="2"/><title>Noise</title>
                    <circle cx="340" cy="180" r="6" fill="#ef4444" stroke="#b91c1c" stroke-width="2"/>
                    <!-- Legend -->
                    <circle cx="60" cy="30" r="6" fill="#22c55e" stroke="#166534" stroke-width="2"/><text x="72" y="35" fill="#334155" font-size="11">Core point</text>
                    <circle cx="170" cy="30" r="5" fill="#f59e0b" stroke="#92400e" stroke-width="1.5"/><text x="180" y="35" fill="#334155" font-size="11">Border point</text>
                    <circle cx="280" cy="30" r="6" fill="#ef4444" stroke="#b91c1c" stroke-width="2"/><text x="292" y="35" fill="#334155" font-size="11">Noise (-1)</text>
                </svg>
                <figcaption>Green = core points (enough neighbors). Yellow = border (near a core). Red = noise/outliers (too far from everything).</figcaption>
            </div>

            <div class="example-box">
                <h4>üó∫Ô∏è Real-World Example: Finding Fraud Clusters</h4>
                <p><strong>Problem:</strong> A bank has 1 million transactions. Most are normal, but some are fraudulent. Fraud transactions form small, dense clusters (e.g. same ATM, same time window), while isolated odd transactions are just noise.</p>
                <p style="margin-top: 10px;"><strong>Why DBSCAN?</strong> K-Means would force every transaction into a cluster ‚Äî even the isolated weird ones. DBSCAN says "these 15 transactions form a suspicious cluster, and <em>these 3 lonely transactions are just noise</em>." The bank investigates only the real clusters.</p>
            </div>

            <h3>Python Code ‚Äî Full Example</h3>
            <div class="code-block">
<pre><span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt
<span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler
<span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons

<span class="comment"># --- Step 1: Generate data with two crescent-moon shapes ---</span>
X, _ = <span class="function">make_moons</span>(n_samples=<span class="number">300</span>, noise=<span class="number">0.08</span>, random_state=<span class="number">42</span>)

<span class="comment"># Add some outliers</span>
outliers = np.array([[<span class="number">-1.5</span>, <span class="number">0.8</span>], [<span class="number">2.5</span>, <span class="number">-0.5</span>], [<span class="number">0.5</span>, <span class="number">1.5</span>]])
X = np.<span class="function">vstack</span>([X, outliers])

<span class="comment"># --- Step 2: Scale the data ---</span>
scaler = <span class="function">StandardScaler</span>()
X_scaled = scaler.<span class="function">fit_transform</span>(X)

<span class="comment"># --- Step 3: Run DBSCAN ---</span>
dbscan = <span class="function">DBSCAN</span>(eps=<span class="number">0.3</span>, min_samples=<span class="number">5</span>)
labels = dbscan.<span class="function">fit_predict</span>(X_scaled)

<span class="comment"># --- Step 4: Analyze results ---</span>
n_clusters = len(<span class="function">set</span>(labels)) - (<span class="number">1</span> <span class="keyword">if</span> -<span class="number">1</span> <span class="keyword">in</span> labels <span class="keyword">else</span> <span class="number">0</span>)
n_noise = <span class="function">list</span>(labels).<span class="function">count</span>(-<span class="number">1</span>)
<span class="function">print</span>(<span class="string">f"Clusters found: {n_clusters}"</span>)
<span class="function">print</span>(<span class="string">f"Noise points: {n_noise}"</span>)

<span class="comment"># --- Step 5: Visualize ---</span>
plt.<span class="function">figure</span>(figsize=(<span class="number">10</span>, <span class="number">6</span>))
<span class="comment"># Cluster points</span>
plt.<span class="function">scatter</span>(X_scaled[labels != -<span class="number">1</span>, <span class="number">0</span>], X_scaled[labels != -<span class="number">1</span>, <span class="number">1</span>],
           c=labels[labels != -<span class="number">1</span>], cmap=<span class="string">'viridis'</span>, s=<span class="number">40</span>, label=<span class="string">'Clusters'</span>)
<span class="comment"># Noise points (red X)</span>
plt.<span class="function">scatter</span>(X_scaled[labels == -<span class="number">1</span>, <span class="number">0</span>], X_scaled[labels == -<span class="number">1</span>, <span class="number">1</span>],
           c=<span class="string">'red'</span>, marker=<span class="string">'x'</span>, s=<span class="number">100</span>, linewidths=<span class="number">2</span>, label=<span class="string">'Noise'</span>)
plt.<span class="function">title</span>(<span class="string">'DBSCAN: Two Moon Shapes + Outliers Detected'</span>)
plt.<span class="function">legend</span>()
plt.<span class="function">show</span>()</pre>
            </div>

            <h3>How to Choose eps and min_samples</h3>
            <p>This is the hardest part. Here are practical tips:</p>
            <ul style="margin-left: 22px; line-height: 1.9;">
                <li><strong>min_samples:</strong> A good default is <code>2 * number_of_features</code>. For 2D data ‚Üí 4 or 5.</li>
                <li><strong>eps:</strong> Use the <strong>k-distance graph</strong>: for each point, compute the distance to its k-th nearest neighbor (k = min_samples). Sort these distances and plot them. Look for an "elbow" ‚Äî that's your eps.</li>
            </ul>
            <div class="code-block">
<pre><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> NearestNeighbors

<span class="comment"># k-distance graph to find eps</span>
k = <span class="number">5</span>  <span class="comment"># same as min_samples</span>
nn = <span class="function">NearestNeighbors</span>(n_neighbors=k)
nn.<span class="function">fit</span>(X_scaled)
distances, _ = nn.<span class="function">kneighbors</span>(X_scaled)

<span class="comment"># Sort the k-th nearest neighbor distances</span>
k_distances = np.<span class="function">sort</span>(distances[:, k-<span class="number">1</span>])

plt.<span class="function">figure</span>(figsize=(<span class="number">8</span>, <span class="number">4</span>))
plt.<span class="function">plot</span>(k_distances)
plt.<span class="function">xlabel</span>(<span class="string">'Points (sorted by distance)'</span>)
plt.<span class="function">ylabel</span>(<span class="string">f'{k}-th nearest neighbor distance'</span>)
plt.<span class="function">title</span>(<span class="string">'k-Distance Graph (find the elbow for eps)'</span>)
plt.<span class="function">axhline</span>(y=<span class="number">0.3</span>, color=<span class="string">'red'</span>, linestyle=<span class="string">'--'</span>, label=<span class="string">'eps = 0.3'</span>)
plt.<span class="function">legend</span>()
plt.<span class="function">show</span>()</pre>
            </div>

            <div class="key-point">
                <h4>ü§î When to Use DBSCAN Instead of K-Means</h4>
                <ul style="margin-left: 20px; color: #3730a3;">
                    <li>Your clusters are <strong>non-spherical</strong> (crescent moons, rings, irregular blobs).</li>
                    <li>You have <strong>outliers/noise</strong> that shouldn't be forced into a cluster.</li>
                    <li>You <strong>don't know K</strong> and don't want to guess.</li>
                    <li>Cluster sizes are <strong>very different</strong> (one big cluster, one tiny one).</li>
                </ul>
            </div>
        </div>

        <!-- Part 6: Algorithm Comparison -->
        <div class="section">
            <h2><i class="fas fa-balance-scale"></i> Part 6: Which Algorithm Should I Use?</h2>
            <p>Here's a decision guide. Think about your data and pick the right tool:</p>

            <table class="data-table">
                <tr>
                    <th>Algorithm</th>
                    <th>Cluster Shape</th>
                    <th>Handles Outliers</th>
                    <th>Need K?</th>
                    <th>Speed</th>
                    <th>Best For</th>
                </tr>
                <tr>
                    <td><strong>K-Means</strong></td>
                    <td>Spherical / round blobs</td>
                    <td>No (forces all points into clusters)</td>
                    <td>Yes</td>
                    <td>Very fast</td>
                    <td>Large data, well-separated round clusters</td>
                </tr>
                <tr>
                    <td><strong>Hierarchical</strong></td>
                    <td>Any (depends on linkage)</td>
                    <td>No</td>
                    <td>Optional (cut dendrogram)</td>
                    <td>Slow for large data</td>
                    <td>Small-medium data, exploring hierarchy</td>
                </tr>
                <tr>
                    <td><strong>DBSCAN</strong></td>
                    <td>Any shape!</td>
                    <td>Yes! (labels them -1)</td>
                    <td>No</td>
                    <td>Medium</td>
                    <td>Irregular shapes, outlier detection</td>
                </tr>
            </table>

            <div class="analogy-box">
                <h4>üéØ Quick Decision Tree</h4>
                <p><strong>Do you know how many clusters?</strong> ‚Üí Yes ‚Üí <strong>K-Means</strong> (fast and simple).</p>
                <p><strong>Do you have outliers?</strong> ‚Üí Yes ‚Üí <strong>DBSCAN</strong> (labels noise).</p>
                <p><strong>Want to explore cluster hierarchy?</strong> ‚Üí Yes ‚Üí <strong>Hierarchical</strong> (dendrogram).</p>
                <p><strong>Data is huge (millions of rows)?</strong> ‚Üí <strong>K-Means</strong> (fastest).</p>
                <p><strong>Clusters are weird shapes?</strong> ‚Üí <strong>DBSCAN</strong>.</p>
            </div>
        </div>

        <!-- Interactive: Test yourself -->
        <div class="section">
            <h2><i class="fas fa-hand-pointer"></i> Test Yourself</h2>
            <p>Check your understanding. Click an answer ‚Äì you‚Äôll get instant feedback.</p>
            <div class="interactive-quiz">
                <h4>1. What does ‚Äúintra-cluster distance‚Äù mean?</h4>
                <label class="quiz-option" data-quiz="q1" data-correct="false">
                    <input type="radio" name="q1"> The distance between different cluster centroids
                </label>
                <label class="quiz-option" data-quiz="q1" data-correct="true">
                    <input type="radio" name="q1"> The distance of points inside a cluster to their own centroid
                </label>
                <label class="quiz-option" data-quiz="q1" data-correct="false">
                    <input type="radio" name="q1"> The number of points in each cluster
                </label>
                <p id="fb-q1" class="quiz-feedback"></p>
            </div>
            <div class="interactive-quiz">
                <h4>2. K-Means is sensitive to outliers. What can you do?</h4>
                <label class="quiz-option" data-quiz="q2" data-correct="true">
                    <input type="radio" name="q2"> Remove or cap outliers, or use RobustScaler / DBSCAN
                </label>
                <label class="quiz-option" data-quiz="q2" data-correct="false">
                    <input type="radio" name="q2"> Ignore them ‚Äì K-Means handles them automatically
                </label>
                <label class="quiz-option" data-quiz="q2" data-correct="false">
                    <input type="radio" name="q2"> Use more clusters (higher K) to absorb outliers
                </label>
                <p id="fb-q2" class="quiz-feedback"></p>
            </div>
            <div class="interactive-quiz">
                <h4>3. In the Elbow method, what is on the Y-axis?</h4>
                <label class="quiz-option" data-quiz="q3" data-correct="false">
                    <input type="radio" name="q3"> Number of clusters (K)
                </label>
                <label class="quiz-option" data-quiz="q3" data-correct="true">
                    <input type="radio" name="q3"> Inertia (WCSS ‚Äì within-cluster sum of squares)
                </label>
                <label class="quiz-option" data-quiz="q3" data-correct="false">
                    <input type="radio" name="q3"> Silhouette score
                </label>
                <p id="fb-q3" class="quiz-feedback"></p>
            </div>
            <div class="interactive-quiz">
                <h4>4. Good clustering has _____ intra-cluster distance and _____ inter-cluster distance.</h4>
                <label class="quiz-option" data-quiz="q4" data-correct="true">
                    <input type="radio" name="q4"> Low (tight groups); high (well separated)
                </label>
                <label class="quiz-option" data-quiz="q4" data-correct="false">
                    <input type="radio" name="q4"> High; low
                </label>
                <label class="quiz-option" data-quiz="q4" data-correct="false">
                    <input type="radio" name="q4"> Low; low
                </label>
                <p id="fb-q4" class="quiz-feedback"></p>
            </div>
        </div>

        <!-- Part 5: Customer Segmentation Example -->
        <div class="section">
            <h2><i class="fas fa-users"></i> Part 5: Real Example - Customer Segmentation</h2>

            <div class="code-block">
<pre><span class="comment"># Customer Segmentation Example</span>
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># Sample customer data</span>
customers = pd.<span class="function">DataFrame</span>({
    <span class="string">'CustomerID'</span>: <span class="function">range</span>(<span class="number">1</span>, <span class="number">101</span>),
    <span class="string">'Annual_Income'</span>: np.random.<span class="function">randint</span>(<span class="number">20000</span>, <span class="number">150000</span>, <span class="number">100</span>),
    <span class="string">'Spending_Score'</span>: np.random.<span class="function">randint</span>(<span class="number">1</span>, <span class="number">100</span>, <span class="number">100</span>),
    <span class="string">'Purchase_Frequency'</span>: np.random.<span class="function">randint</span>(<span class="number">1</span>, <span class="number">50</span>, <span class="number">100</span>)
})

<span class="comment"># Prepare features</span>
X = customers[[<span class="string">'Annual_Income'</span>, <span class="string">'Spending_Score'</span>, <span class="string">'Purchase_Frequency'</span>]]

<span class="comment"># Scale features</span>
scaler = <span class="function">StandardScaler</span>()
X_scaled = scaler.<span class="function">fit_transform</span>(X)

<span class="comment"># Find optimal K</span>
<span class="keyword">for</span> k <span class="keyword">in</span> <span class="function">range</span>(<span class="number">2</span>, <span class="number">7</span>):
    kmeans = <span class="function">KMeans</span>(n_clusters=k, random_state=<span class="number">42</span>, n_init=<span class="number">10</span>)
    labels = kmeans.<span class="function">fit_predict</span>(X_scaled)
    score = <span class="function">silhouette_score</span>(X_scaled, labels)
    <span class="function">print</span>(<span class="string">f"K={k}: Silhouette = {score:.3f}"</span>)

<span class="comment"># Apply final clustering</span>
kmeans = <span class="function">KMeans</span>(n_clusters=<span class="number">4</span>, random_state=<span class="number">42</span>, n_init=<span class="number">10</span>)
customers[<span class="string">'Segment'</span>] = kmeans.<span class="function">fit_predict</span>(X_scaled)

<span class="comment"># Analyze segments</span>
segment_analysis = customers.<span class="function">groupby</span>(<span class="string">'Segment'</span>).<span class="function">agg</span>({
    <span class="string">'Annual_Income'</span>: <span class="string">'mean'</span>,
    <span class="string">'Spending_Score'</span>: <span class="string">'mean'</span>,
    <span class="string">'Purchase_Frequency'</span>: <span class="string">'mean'</span>,
    <span class="string">'CustomerID'</span>: <span class="string">'count'</span>
}).<span class="function">rename</span>(columns={<span class="string">'CustomerID'</span>: <span class="string">'Count'</span>})

<span class="function">print</span>(<span class="string">"\nüìä Customer Segments:"</span>)
<span class="function">print</span>(segment_analysis.<span class="function">round</span>(<span class="number">0</span>))</pre>
            </div>

            <div class="example-box">
                <h4>üìä Interpreting the Segments</h4>
                <ul style="margin-left: 20px;">
                    <li><strong>Segment 0 (High Income, High Spending):</strong> VIP customers - target for premium products</li>
                    <li><strong>Segment 1 (Low Income, High Spending):</strong> At risk - may need financial products</li>
                    <li><strong>Segment 2 (High Income, Low Spending):</strong> Potential - target with engagement campaigns</li>
                    <li><strong>Segment 3 (Low Income, Low Spending):</strong> Budget conscious - target with discounts</li>
                </ul>
            </div>

            <div class="important-box" style="background: linear-gradient(135deg, #fef2f2 0%, #fecaca 100%); border-radius: 16px; padding: 24px; margin: 25px 0; border-left: 5px solid #ef4444;">
                <h4 style="color: #b91c1c;">üö´ Common Mistakes in Clustering</h4>
                <ul style="margin-left: 20px; color: #7f1d1d;">
                    <li><strong>Not scaling features</strong> ‚Äî If one feature is 0‚Äì100 and another is 0‚Äì1, distance is dominated by the first. Always scale (e.g. StandardScaler) before K-Means.</li>
                    <li><strong>Assuming K is obvious</strong> ‚Äî There is no "correct" K; use elbow plot, silhouette score, and business sense together.</li>
                    <li><strong>Using K-Means when clusters aren't round</strong> ‚Äî K-Means assumes roughly spherical clusters; for odd shapes or lots of noise, consider DBSCAN or hierarchical.</li>
                    <li><strong>Forgetting that clusters are unlabeled</strong> ‚Äî The algorithm gives you group 0, 1, 2‚Ä¶ you have to interpret what each group means.</li>
                </ul>
            </div>

            <div class="important-box" style="background: linear-gradient(135deg, #eff6ff 0%, #dbeafe 100%); border-radius: 16px; padding: 24px; margin: 25px 0; border-left: 5px solid #3b82f6;">
                <h4 style="color: #1e40af;">üìò From the course notebook (Clustering)</h4>
                <p style="color: #1e3a8a; margin-bottom: 10px;">The course source uses <strong>Hotel Reservations.csv</strong>: <code>data = pd.read_csv("Hotel Reservations.csv")</code>. Use it to cluster customer bookings (e.g. select numeric columns like lead_time, avg_price_per_room, no_of_weekend_nights). Scale with <code>StandardScaler</code>, then <code>KMeans(n_clusters=k).fit(X)</code>; try elbow plot or silhouette to pick k. Download <a href="datasets/hotel_reservations.csv" download="Hotel Reservations.csv">Hotel Reservations.csv</a> from the <a href="datasets.html">datasets page</a>. See <strong>Clustering.pdf</strong> in the course source for slides.</p>
            </div>

            <div class="section" style="margin-top: 2rem;">
                <h2><i class="fas fa-code"></i> Complete code from course notebook: kmeans_clustering.ipynb</h2>
                <p>Every line of code from the course notebook is below (verbatim). Comments may be explained elsewhere; the code is unchanged.</p>
                <div class="code-block" style="max-height: 600px; overflow: auto;">
                    <pre style="white-space: pre; font-size: 0.85em;"># --- Code cell 1 ---
from IPython.core.display import HTML

HTML("""
&lt;style&gt;

h1 { color: blue !important; }
h2 { color: green !important; }
&lt;/style&gt;
""")

# --- Code cell 2 ---
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# --- Code cell 3 ---
data =  pd.read_csv("Hotel Reservations.csv")

# --- Code cell 4 ---
# We have a data of hotel reservations
# Use it for clustering customer bookings to identify patterns

# --- Code cell 7 ---
data.head(100)

# --- Code cell 8 ---
data.info()

# --- Code cell 9 ---
data.describe(include ='all')

# --- Code cell 10 ---
print(data['type_of_meal_plan'].value_counts())

# --- Code cell 11 ---
print(data['room_type_reserved'].value_counts())

# --- Code cell 12 ---
print(data['market_segment_type'].value_counts())

# --- Code cell 13 ---
print(data['booking_status'].value_counts())

# --- Code cell 14 ---
print(data['required_car_parking_space'].value_counts())

# --- Code cell 15 ---
print(data['repeated_guest'].value_counts())

# --- Code cell 16 ---
print(data['no_of_previous_bookings_not_canceled'].value_counts().head(10))
# As there are only ~3% repeat customers using previous booking data is not significant

# --- Code cell 17 ---
print(data['no_of_adults'].value_counts())

# --- Code cell 18 ---
print(data['no_of_children'].value_counts())
#Number of children 9 and 10 looks like outlier

# --- Code cell 19 ---
#oulier removal
data = data[data['no_of_children']&lt;=3]
data.reset_index(inplace=True)

# --- Code cell 20 ---
print(data.columns)

# --- Code cell 21 ---
numerical_features = ['no_of_adults', 'no_of_children', 'no_of_weekend_nights',
       'no_of_week_nights', 'required_car_parking_space', 'lead_time','repeated_guest',
       'avg_price_per_room', 'no_of_special_requests']

# --- Code cell 22 ---
categorical_features  = ['type_of_meal_plan','room_type_reserved','market_segment_type']

# --- Code cell 23 ---
data_features = data[numerical_features + categorical_features + ['booking_status']  ]
data_features.head(10)

# --- Code cell 24 ---
data.repeated_guest.value_counts()

# --- Code cell 27 ---
x_train  = data_features[numerical_features + categorical_features]

# There is nothing to predict in clustering
# We are just storing booking status flag in another variable to 
# check later if the clusters have some pattern w.r.t booking cancellation
y_label = data_features[['booking_status']]

# --- Code cell 28 ---
x_train = pd.get_dummies(x_train, columns =categorical_features, drop_first= False)
print(x_train.columns)

# --- Code cell 29 ---
x_train.head(10)

# --- Code cell 32 ---
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
x_train[numerical_features] = scaler.fit_transform(x_train[numerical_features])
x_train.describe()
x_train_copy = x_train.copy()

# --- Code cell 33 ---
x_train

# --- Code cell 34 ---
x_train_copy.to_csv("x_train.csv",index= False)

# --- Code cell 36 ---
from sklearn.cluster import KMeans


kmeans = KMeans(n_clusters=10, random_state=0, n_init="auto").fit(x_train)
x_train['cluster_labels'] = kmeans.labels_
x_train['booking_status'] = y_label['booking_status']

# --- Code cell 37 ---
from sklearn.metrics import silhouette_score
silhouette_score(x_train_copy, kmeans.labels_)

# --- Code cell 38 ---
x_train['cluster_labels'].value_counts()

# --- Code cell 41 ---
x_train['booking_status'].value_counts()

# --- Code cell 42 ---
print("cancellation rate in data:", 100*11884/(11884 + 24388))

# --- Code cell 43 ---
cluster_number = []
cancellation_rate = []

for z in range(len(list(x_train['cluster_labels'].unique()))):
               cluster_number.append(z)
               temp = x_train[x_train['cluster_labels']==z]
               temp_cancelled = temp[temp['booking_status']=='Canceled']
               temp_not_cancelled = temp[temp['booking_status']=='Not_Canceled']
               cancel = (len(temp_cancelled)/len(temp))*100
               cancellation_rate.append(cancel)

# --- Code cell 44 ---
temp = pd.DataFrame({'cluster':cluster_number, 'cancellation': cancellation_rate})
sns.barplot(x = 'cluster',y = 'cancellation', data = temp)

# --- Code cell 46 ---
data['cluster'] = kmeans.labels_
data.to_csv('clustering_results.csv')

# --- Code cell 48 ---
# Check average value of numerical features across clusters

# --- Code cell 49 ---

plt.figure(figsize=(20, 12))

plt.subplot(3,3,1)
temp = pd.DataFrame(data.groupby('cluster')['no_of_adults'].mean()).reset_index()
sns.barplot(x = 'cluster',y = 'no_of_adults', data = temp)

plt.subplot(3,3,2)
temp = pd.DataFrame(data.groupby('cluster')['no_of_children'].mean()).reset_index()
sns.barplot(x = 'cluster',y = 'no_of_children', data = temp)

plt.subplot(3,3,3)
temp = pd.DataFrame(data.groupby('cluster')['no_of_weekend_nights'].mean()).reset_index()
sns.barplot(x = 'cluster',y = 'no_of_weekend_nights',data = temp)

plt.subplot(3,3,4)
temp = pd.DataFrame(data.groupby('cluster')['no_of_week_nights'].mean()).reset_index()
sns.barplot(x = 'cluster',y = 'no_of_week_nights', data = temp)

plt.subplot(3,3,5)
temp = pd.DataFrame(data.groupby('cluster')['required_car_parking_space'].mean()).reset_index()
sns.barplot(x = 'cluster',y = 'required_car_parking_space', data = temp)

plt.subplot(3,3,6)
temp = pd.DataFrame(data.groupby('cluster')['lead_time'].mean()).reset_index()
sns.barplot(x = 'cluster',y = 'lead_time',data = temp)

plt.subplot(3,3,7)
temp = pd.DataFrame(data.groupby('cluster')['avg_price_per_room'].mean()).reset_index()
sns.barplot(x = 'cluster',y = 'avg_price_per_room', data = temp)

plt.subplot(3,3,8)
temp = pd.DataFrame(data.groupby('cluster')['no_of_special_requests'].mean()).reset_index()
sns.barplot(x = 'cluster',y = 'no_of_special_requests', data = temp)

plt.show()

# --- Code cell 51 ---
# Check frequency of categorical features across clusters

# --- Code cell 52 ---

plt.figure(figsize=(20, 12))

plt.subplot(2,2,1)
sns.countplot(x='cluster', hue='type_of_meal_plan', data=data)

plt.subplot(2,2,2)
sns.countplot(x='cluster', hue='room_type_reserved', data=data)

plt.subplot(2,2,3)
sns.countplot(x='cluster', hue='market_segment_type', data=data)

plt.subplot(2,2,4)
sns.countplot(x='cluster', hue='repeated_guest', data=data)

plt.show()

# --- Code cell 55 ---
from sklearn.cluster import KMeans
wcss = [] 

for i in range(2, 30): 
    kmeans = KMeans(n_clusters = i, init = 'k-means++', n_init= 'auto', random_state = 42)
    kmeans.fit(x_train_copy) 
    wcss.append(kmeans.inertia_)

# --- Code cell 57 ---
import matplotlib.pyplot as plt
K = range(2, 30)
plt.plot(K, wcss, 'bx-')
plt.xlabel('Values of K')
plt.ylabel('Within cluster Sum of Squared distances')
plt.title('The Elbow Method')
plt.show()</pre>
                </div>
            </div>

            <div class="section" style="margin-top: 2rem;">
                <h2><i class="fas fa-code"></i> Complete code from course notebook: agglomerative_clustering.ipynb</h2>
                <p>Every line of code from the course notebook (verbatim).</p>
                <div class="code-block" style="max-height: 400px; overflow: auto;">
                    <pre style="white-space: pre; font-size: 0.85em;"># --- Code cell 1 ---
import pandas as pd

# --- Code cell 2 ---
# read original data and feature matrix

# --- Code cell 3 ---
x_train = pd.read_csv("x_train.csv")
data =  pd.read_csv("Hotel Reservations.csv")

# --- Code cell 4 ---
# reduce the number of rows in data if you face memory issues
# This is just to see end to end execution - not a recommended step

x_train = x_train[0:10000]
data = data[0:10000]

# --- Code cell 5 ---
x_train_copy = x_train.copy()

# --- Code cell 7 ---
# Try agglomerative clustering with cosine distance metric and distance threshold as input
# You can also specify n_clusters and set distance_threshold to None
# You can try different distance metrics and linkage criterias

# --- Code cell 8 ---
from sklearn.cluster import AgglomerativeClustering


clustering = AgglomerativeClustering( n_clusters = None,
                                      linkage = 'complete',
                                      distance_threshold = 0.5, # if n_clusters is number then this should be None
                                      metric = 'cosine')

clustering.fit(x_train)
x_train['cluster_labels'] = clustering.labels_
x_train['booking_status'] = data['booking_status']
print(x_train['cluster_labels'].value_counts())

# --- Code cell 10 ---
# get cancellation rate in each cluster

# --- Code cell 11 ---
cluster_number = []
cancellation_rate = []

for z in range(len(list(x_train['cluster_labels'].unique()))):
               cluster_number.append(z)
               temp = x_train[x_train['cluster_labels']==z]
               temp_cancelled = temp[temp['booking_status']=='Canceled']
               temp_not_cancelled = temp[temp['booking_status']=='Not_Canceled']
               cancel = (len(temp_cancelled)/len(temp))*100
               cancellation_rate.append(cancel)

# --- Code cell 13 ---
import seaborn as sns
temp = pd.DataFrame({'cluster':cluster_number, 'cancellation': cancellation_rate})
sns.barplot(x = 'cluster',y = 'cancellation', data = temp)

# --- Code cell 15 ---
# get silehoutee score

# --- Code cell 16 ---
from sklearn.metrics import silhouette_score
silhouette_score(x_train_copy, clustering.labels_)</pre>
                </div>
            </div>

            <div class="reflection-prompt">
                <h4>üí≠ Short reflection</h4>
                <p>In one sentence: why would you choose DBSCAN over K-Means when your dataset has many outliers or oddly shaped clusters? (Think: density, noise, and not having to pick K.)</p>
            </div>
        </div>

        <!-- Core & Non-Core Points (Mastery Checklist) -->
        <div class="section">
            <h2><i class="fas fa-star"></i> Core & Non-Core Points ‚Äì Mastery Checklist</h2>
            <p>To become a <strong>master</strong> of clustering, you must know every core point. Non-core points make you stand out in interviews and real projects.</p>
            
            <div class="core-box">
                <h4>‚úÖ CORE (Must know ‚Äì exam & job essentials)</h4>
                <ul>
                    <li>Clustering is <strong>unsupervised</strong> ‚Äì no labels; algorithm finds groups.</li>
                    <li><strong>K-Means</strong>: choose K, initialize K centroids, assign each point to nearest centroid, update centroid to mean of its points, repeat until convergence.</li>
                    <li><strong>Intra-cluster distance (WCSS/Inertia)</strong>: sum of squared distances of points to their centroid; minimize it.</li>
                    <li><strong>Inter-cluster distance</strong>: distance between clusters; good clustering = low intra, high inter.</li>
                    <li><strong>Choosing K</strong>: Elbow method (plot inertia vs K; pick the elbow) and Silhouette score (higher = better).</li>
                    <li><strong>Always scale</strong> features before K-Means (distance-based).</li>
                    <li>K-Means is <strong>sensitive to outliers</strong>; handle with removal, capping, RobustScaler, or DBSCAN.</li>
                    <li><strong>DBSCAN</strong>: density-based; finds arbitrary shapes and labels outliers as -1; no need to set K.</li>
                    <li><strong>Hierarchical clustering</strong>: dendrogram; can cut at desired K; good for small/medium data.</li>
                    <li>Compare algorithms: K-Means (spherical, needs K), Hierarchical (any shape, optional K), DBSCAN (any shape, outliers, no K).</li>
                </ul>
            </div>
            <div class="noncore-box">
                <h4>üìö NON-CORE (Good to know ‚Äì depth & interviews)</h4>
                <ul>
                    <li><strong>Convergence</strong>: stop when centroid movement is below threshold or max iterations reached.</li>
                    <li><strong>K-Means++</strong>: smarter initialization; reduces bad local minima and improves stability.</li>
                    <li><strong>Distance metrics</strong>: Euclidean (default), Manhattan for grid-like or high-dimensional data.</li>
                    <li><strong>Silhouette interpretation</strong>: +1 (ideal), 0 (boundary), -1 (wrong cluster).</li>
                    <li><strong>Multiple runs</strong>: run K-Means with different random seeds and pick the best inertia or silhouette.</li>
                    <li><strong>Dendrogram</strong>: read height as ‚Äúdistance when clusters merge‚Äù; cut where gap is large.</li>
                    <li><strong>DBSCAN parameters</strong>: <code>eps</code> (max distance for neighbors), <code>min_samples</code> (min points to form core).</li>
                    <li><strong>GMM (Gaussian Mixture)</strong>: soft clustering (probabilities); can capture ellipsoidal clusters.</li>
                    <li>Use clustering for <strong>exploration</strong> (discover segments) and <strong>preprocessing</strong> (e.g. cluster then model per cluster).</li>
                </ul>
            </div>
        </div>

        <!-- Summary -->
        <div class="section">
            <h2><i class="fas fa-graduation-cap"></i> Summary</h2>
            
            <table class="data-table">
                <tr>
                    <th>Concept</th>
                    <th>Key Points</th>
                </tr>
                <tr>
                    <td><strong>Clustering</strong></td>
                    <td>Unsupervised learning - finds natural groups without labels</td>
                </tr>
                <tr>
                    <td><strong>K-Means</strong></td>
                    <td>Fast, simple; finds K spherical clusters using centroids</td>
                </tr>
                <tr>
                    <td><strong>Choosing K</strong></td>
                    <td>Elbow method (inertia) or Silhouette score</td>
                </tr>
                <tr>
                    <td><strong>DBSCAN</strong></td>
                    <td>Density-based; finds any shape, detects outliers</td>
                </tr>
                <tr>
                    <td><strong>Scaling</strong></td>
                    <td>ALWAYS scale your data before clustering!</td>
                </tr>
            </table>

            <div class="key-point">
                <h4>üéØ Pro Tips</h4>
                <ul style="margin-left: 20px; color: #3730a3;">
                    <li><strong>Always scale features</strong> - clustering is distance-based!</li>
                    <li><strong>Try multiple K values</strong> - use both elbow and silhouette</li>
                    <li><strong>Visualize results</strong> - even if just 2D PCA plot</li>
                    <li><strong>Interpret clusters</strong> - give them meaningful business names</li>
                </ul>
            </div>
        </div>

        <div style="background: linear-gradient(135deg, #eff6ff 0%, #dbeafe 100%); border-radius: 16px; padding: 24px; margin: 30px 0; border: 2px solid #bfdbfe; text-align: center;">
            <h3 style="color: #1e40af; margin-bottom: 10px;"><i class="fas fa-code"></i> Want to see the code?</h3>
            <p style="color: #1e3a8a; margin-bottom: 16px;">Every single line of the K-Means notebook explained like you are 5 years old.</p>
            <a href="kmeans-code-walkthrough.html" style="display: inline-block; background: linear-gradient(135deg, #7c3aed, #db2777); color: white; padding: 14px 28px; border-radius: 12px; text-decoration: none; font-weight: 700; font-size: 1.05em;"><i class="fas fa-arrow-right"></i> K-Means Code Walkthrough</a>
        </div>

        <div class="nav-buttons">
            <a href="boosting.html" class="nav-btn prev"><i class="fas fa-arrow-left"></i> Previous: Boosting</a>
            <a href="deep-learning.html" class="nav-btn next">Next: Deep Learning <i class="fas fa-arrow-right"></i></a>
        </div>
    </div>

    <button class="back-to-top" id="backToTop"><i class="fas fa-arrow-up"></i></button>
    <script>
        const backToTopButton = document.getElementById('backToTop');
        window.addEventListener('scroll', () => {
            if (window.pageYOffset > 300) backToTopButton.classList.add('show');
            else backToTopButton.classList.remove('show');
        });
        backToTopButton.addEventListener('click', () => window.scrollTo({ top: 0, behavior: 'smooth' }));

        // Interactive quiz: on option click, show correct/wrong and feedback
        document.querySelectorAll('.quiz-option').forEach(function(opt) {
            opt.addEventListener('click', function() {
                var quizId = this.getAttribute('data-quiz');
                var correct = this.getAttribute('data-correct') === 'true';
                var container = this.closest('.interactive-quiz');
                container.querySelectorAll('.quiz-option').forEach(function(o) {
                    o.classList.remove('correct', 'wrong');
                    if (o.getAttribute('data-correct') === 'true') o.classList.add('correct');
                    else if (o !== opt && opt.getAttribute('data-correct') !== 'true') { }
                });
                this.classList.add(correct ? 'correct' : 'wrong');
                var fb = document.getElementById('fb-' + quizId);
                if (fb) {
                    fb.textContent = correct ? '‚úì Correct! Well done.' : '‚úó Not quite. Review the section and try again.';
                    fb.className = 'quiz-feedback show ' + (correct ? 'correct-msg' : 'wrong-msg');
                }
            });
        });
    </script>
    <script src="../js/code-copy.js"></script>
</body>
</html>
