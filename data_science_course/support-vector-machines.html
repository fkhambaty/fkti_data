<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Support Vector Machines (SVM) | Fakhruddin Khambaty's Learning Hub</title>
    <link href="https://fonts.googleapis.com/css2?family=Nunito:wght@400;500;600;700;800&family=Fira+Code:wght@400;500&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css" rel="stylesheet">
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Nunito', sans-serif;
            background: linear-gradient(135deg, #fdf4ff 0%, #fae8ff 50%, #f5d0fe 100%);
            min-height: 100vh;
            padding: 20px;
            color: #1e293b;
            line-height: 2;
            font-size: 18px;
        }
        .container { max-width: 900px; margin: 0 auto; }
        .nav {
            background: rgba(255, 255, 255, 0.95);
            backdrop-filter: blur(10px);
            padding: 15px 30px;
            border-radius: 15px;
            margin-bottom: 30px;
            box-shadow: 0 4px 20px rgba(0,0,0,0.08);
            display: flex;
            justify-content: space-between;
            align-items: center;
            flex-wrap: wrap;
            gap: 12px;
        }
        .nav a { color: #9333ea; text-decoration: none; font-weight: 600; display: flex; align-items: center; gap: 8px; }
        .nav a:hover { color: #7e22ce; }
        .header {
            text-align: center;
            padding: 50px 40px;
            background: linear-gradient(135deg, #a855f7 0%, #9333ea 50%, #7e22ce 100%);
            border-radius: 25px;
            color: white;
            margin-bottom: 40px;
            box-shadow: 0 10px 40px rgba(147, 51, 234, 0.3);
        }
        .header h1 { font-size: 2.5em; margin-bottom: 15px; font-weight: 800; }
        .header p { font-size: 1.2em; opacity: 0.95; max-width: 700px; margin: 0 auto; }
        .beginner-badge {
            background: #f59e0b;
            color: white;
            padding: 8px 20px;
            border-radius: 25px;
            font-weight: 700;
            display: inline-block;
            margin-bottom: 20px;
            font-size: 0.9em;
        }
        .section {
            background: white;
            border-radius: 25px;
            padding: 45px;
            margin-bottom: 35px;
            box-shadow: 0 4px 25px rgba(0,0,0,0.08);
            border: 3px solid #f0abfc;
        }
        .section h2 { color: #9333ea; font-size: 1.8em; margin-bottom: 25px; display: flex; align-items: center; gap: 15px; padding-bottom: 15px; border-bottom: 3px solid #f5d0fe; }
        .section h3 { color: #7e22ce; font-size: 1.4em; margin: 35px 0 20px 0; padding-left: 20px; border-left: 5px solid #a855f7; }
        .section p { font-size: 1.1em; color: #334155; margin-bottom: 20px; }
        .eli5-box {
            background: linear-gradient(135deg, #fef3c7 0%, #fde68a 100%);
            border-radius: 20px;
            padding: 30px;
            margin: 25px 0;
            border: 3px dashed #f59e0b;
        }
        .eli5-box h4 { color: #92400e; font-size: 1.3em; margin-bottom: 15px; }
        .eli5-box p { color: #78350f; font-size: 1.15em; margin-bottom: 10px; }
        .analogy-box {
            background: linear-gradient(135deg, #faf5ff 0%, #f3e8ff 100%);
            border-radius: 20px;
            padding: 30px;
            margin: 25px 0;
            border-left: 5px solid #9333ea;
        }
        .analogy-box h4 { color: #7e22ce; font-size: 1.2em; margin-bottom: 15px; }
        .analogy-box p, .analogy-box li { color: #581c87; }
        .key-point {
            background: linear-gradient(135deg, #fdf4ff 0%, #fae8ff 100%);
            border-radius: 20px;
            padding: 25px;
            margin: 25px 0;
            border-left: 5px solid #a855f7;
        }
        .key-point h4 { color: #7e22ce; margin-bottom: 12px; }
        .key-point ul { margin-left: 22px; color: #581c87; }
        .key-point li { margin-bottom: 8px; }
        .warning-box {
            background: linear-gradient(135deg, #fef2f2 0%, #fecaca 100%);
            border-radius: 20px;
            padding: 25px;
            margin: 25px 0;
            border: 3px solid #ef4444;
        }
        .warning-box h4 { color: #b91c1c; margin-bottom: 10px; }
        .warning-box p { color: #991b1b; }
        .code-block {
            background: #1e293b;
            border-radius: 20px;
            padding: 30px;
            margin: 25px 0;
            overflow-x: auto;
        }
        .code-block pre { margin: 0; font-family: 'Fira Code', monospace; font-size: 0.95em; color: #e2e8f0; line-height: 1.8; }
        .code-block .comment { color: #94a3b8; }
        .code-block .keyword { color: #c084fc; }
        .code-block .function { color: #38bdf8; }
        .code-block .string { color: #4ade80; }
        .code-block .number { color: #fb923c; }
        .data-table {
            width: 100%;
            border-collapse: collapse;
            margin: 25px 0;
            background: white;
            border-radius: 15px;
            overflow: hidden;
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        .data-table th { background: linear-gradient(135deg, #a855f7 0%, #9333ea 100%); color: white; padding: 18px 15px; text-align: left; }
        .data-table td { padding: 15px; border-bottom: 2px solid #f1f5f9; }
        .data-table tr:nth-child(even) { background: #faf5ff; }
        .step-card {
            display: flex;
            align-items: flex-start;
            gap: 20px;
            background: #faf5ff;
            padding: 22px 25px;
            border-radius: 15px;
            margin: 15px 0;
            border: 2px solid #f0abfc;
        }
        .step-num {
            width: 44px; height: 44px;
            background: linear-gradient(135deg, #a855f7, #9333ea);
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            color: white;
            flex-shrink: 0;
        }
        .step-card h5 { color: #7e22ce; font-size: 1.15em; margin-bottom: 6px; }
        .step-card p { color: #475569; margin: 0; font-size: 1em; }
        .svm-visual {
            background: linear-gradient(135deg, #faf5ff 0%, #f3e8ff 100%);
            border-radius: 20px;
            padding: 30px;
            margin: 25px 0;
            border: 3px solid #a855f7;
            text-align: center;
        }
        .svm-visual h4 { color: #7e22ce; margin-bottom: 15px; }
        .svm-visual svg { max-width: 100%; height: auto; }
        .nav-buttons { display: flex; justify-content: space-between; margin-top: 50px; gap: 20px; flex-wrap: wrap; }
        .nav-btn { display: inline-flex; align-items: center; gap: 10px; padding: 18px 35px; border-radius: 15px; text-decoration: none; font-weight: 700; transition: all 0.3s; }
        .nav-btn.prev { background: #f1f5f9; color: #475569; }
        .nav-btn.next { background: linear-gradient(135deg, #a855f7 0%, #9333ea 100%); color: white; }
        .nav-btn:hover { transform: translateY(-3px); box-shadow: 0 8px 25px rgba(0,0,0,0.15); }
        .back-to-top { position: fixed; bottom: 30px; right: 30px; width: 55px; height: 55px; background: linear-gradient(135deg, #a855f7 0%, #9333ea 100%); color: white; border: none; border-radius: 50%; cursor: pointer; display: flex; align-items: center; justify-content: center; font-size: 22px; z-index: 1000; opacity: 0; visibility: hidden; transition: all 0.3s; }
        .back-to-top.show { opacity: 1; visibility: visible; }
        .comparison-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 15px;
            margin: 20px 0;
        }
        .comparison-card {
            background: #faf5ff;
            border: 2px solid #f0abfc;
            border-radius: 15px;
            padding: 20px;
        }
        .comparison-card h5 { color: #9333ea; margin-bottom: 8px; }
        .comparison-card p { margin: 0; font-size: 0.95em; }
        /* Math formula boxes */
        .formula-box {
            background: linear-gradient(135deg, #fefce8 0%, #fef9c3 100%);
            border: 3px solid #eab308;
            border-radius: 20px;
            padding: 30px;
            margin: 25px 0;
            position: relative;
            overflow: hidden;
        }
        .formula-box::before {
            content: 'üìê';
            position: absolute;
            top: 12px;
            right: 15px;
            font-size: 1.5em;
            opacity: 0.3;
        }
        .formula-box h4 { color: #854d0e; font-size: 1.25em; margin-bottom: 18px; display: flex; align-items: center; gap: 10px; }
        .formula-box p { color: #713f12; margin-bottom: 12px; font-size: 1.05em; }
        .formula {
            background: white;
            border: 2px solid #facc15;
            border-radius: 14px;
            padding: 20px 28px;
            margin: 18px 0;
            text-align: center;
            font-size: 1.3em;
            font-weight: 700;
            color: #1e293b;
            line-height: 1.8;
            box-shadow: 0 2px 10px rgba(0,0,0,0.06);
        }
        .formula .frac { display: inline-block; text-align: center; vertical-align: middle; }
        .formula .frac-num { display: block; border-bottom: 2px solid #1e293b; padding: 0 8px 4px; }
        .formula .frac-den { display: block; padding: 4px 8px 0; }
        .formula .highlight { color: #9333ea; }
        .formula .blue { color: #2563eb; }
        .formula .orange { color: #ea580c; }
        .formula .green { color: #16a34a; }
        .formula .small-note { font-size: 0.65em; color: #64748b; font-weight: 500; display: block; margin-top: 6px; }
        .formula-label { display: inline-block; background: #fef08a; color: #854d0e; font-size: 0.75em; font-weight: 700; padding: 3px 12px; border-radius: 20px; margin-bottom: 8px; }
        .layman-row {
            display: grid;
            grid-template-columns: auto 1fr;
            gap: 12px;
            align-items: flex-start;
            margin: 10px 0;
            padding: 12px 15px;
            background: rgba(255,255,255,0.7);
            border-radius: 10px;
        }
        .layman-row .var { font-weight: 800; color: #9333ea; font-size: 1.1em; white-space: nowrap; min-width: 50px; }
        .layman-row .meaning { color: #713f12; font-size: 1em; }
        .worked-example {
            background: linear-gradient(135deg, #ecfdf5 0%, #d1fae5 100%);
            border: 2px solid #22c55e;
            border-radius: 16px;
            padding: 25px;
            margin: 18px 0;
        }
        .worked-example h5 { color: #166534; font-size: 1.1em; margin-bottom: 12px; display: flex; align-items: center; gap: 8px; }
        .worked-example p { color: #14532d; margin-bottom: 8px; }
        .worked-example .calc { font-family: 'Fira Code', monospace; background: white; padding: 10px 16px; border-radius: 10px; margin: 10px 0; font-size: 0.95em; border-left: 4px solid #22c55e; }
        @media (max-width: 768px) {
            body { padding: 10px; font-size: 16px; }
            .header { padding: 30px 20px; }
            .header h1 { font-size: 1.8em; }
            .section { padding: 25px 20px; }
            .nav-buttons { flex-direction: column; }
            .comparison-grid { grid-template-columns: 1fr; }
        }
    </style>
</head>
<body>
    <div class="container">
        <nav class="nav">
            <a href="../index.html"><i class="fas fa-home"></i><span>Home</span></a>
            <a href="k-nearest-neighbors.html"><i class="fas fa-arrow-left"></i><span>Previous: kNN</span></a>
            <a href="index.html"><i class="fas fa-th-large"></i><span>Course Hub</span></a>
        </nav>

        <div class="header">
            <span class="beginner-badge">üë∂ ABSOLUTE BEGINNER FRIENDLY</span>
            <h1>üó°Ô∏è Support Vector Machines (SVM)</h1>
            <p>The algorithm that draws the BEST possible boundary between groups. Think of it as building the widest road between two neighborhoods!</p>
        </div>

        <!-- ======================================== -->
        <!-- Part 1: What is SVM?                     -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-question-circle"></i> Part 1: What is a Support Vector Machine?</h2>

            <p>A Support Vector Machine (SVM) is a <strong>supervised learning algorithm</strong> used for both <strong>classification</strong> and <strong>regression</strong>. Its superpower? It finds the <strong>best possible boundary</strong> (called a <strong>hyperplane</strong>) that separates different classes with the <strong>maximum margin</strong>.</p>

            <h3>üë∂ In One Sentence (Like You're 5)</h3>
            <p><strong>SVM</strong> means: "Draw a line between the red balls and blue balls, but make it as FAR from both groups as possible. That way, even if a new ball wobbles a little, it still ends up on the right side."</p>

            <div class="eli5-box">
                <h4>üè† Imagine This...</h4>
                <p>You have a big playground. On the left side, all the cats hang out. On the right side, all the dogs hang out. You need to build a fence between them.</p>
                <p>You COULD build it right next to the cats (but then a cat might jump over!). You COULD build it right next to the dogs (same problem!).</p>
                <p>The SMARTEST thing? Build the fence <strong>exactly in the middle</strong> so it's as far from BOTH groups as possible. That's what SVM does! It builds the fence (the <strong>hyperplane</strong>) with the <strong>widest possible gap</strong> (the <strong>margin</strong>) between both sides.</p>
                <p>The cats and dogs sitting closest to the fence? Those are the <strong>support vectors</strong>. They're the ones that determine where the fence goes!</p>
            </div>

            <div class="svm-visual">
                <h4>The SVM Concept - Maximum Margin Classifier</h4>
                <svg viewBox="0 0 500 350" xmlns="http://www.w3.org/2000/svg">
                    <!-- Margin area -->
                    <rect x="210" y="20" width="80" height="310" fill="#f3e8ff" stroke="none" opacity="0.6">
                        <animate attributeName="opacity" values="0.3;0.7;0.3" dur="3s" repeatCount="indefinite"/>
                    </rect>

                    <!-- Decision boundary (hyperplane) -->
                    <line x1="250" y1="15" x2="250" y2="335" stroke="#9333ea" stroke-width="3" stroke-dasharray="8,4">
                        <animate attributeName="stroke-opacity" values="0.6;1;0.6" dur="2s" repeatCount="indefinite"/>
                    </line>

                    <!-- Margin lines -->
                    <line x1="210" y1="15" x2="210" y2="335" stroke="#a855f7" stroke-width="1.5" stroke-dasharray="4,4" opacity="0.5"/>
                    <line x1="290" y1="15" x2="290" y2="335" stroke="#a855f7" stroke-width="1.5" stroke-dasharray="4,4" opacity="0.5"/>

                    <!-- Margin arrow -->
                    <line x1="210" y1="340" x2="290" y2="340" stroke="#7e22ce" stroke-width="2"/>
                    <polygon points="210,340 216,336 216,344" fill="#7e22ce"/>
                    <polygon points="290,340 284,336 284,344" fill="#7e22ce"/>
                    <text x="250" y="355" text-anchor="middle" fill="#7e22ce" font-size="11" font-weight="700" font-family="Nunito">MARGIN (as wide as possible!)</text>

                    <!-- Class A points (blue circles - left side) -->
                    <circle cx="80" cy="60" r="14" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="120" cy="120" r="14" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="60" cy="180" r="14" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="140" cy="220" r="14" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="100" cy="280" r="14" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="50" cy="100" r="14" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="160" cy="160" r="14" fill="#3b82f6" opacity="0.8"/>

                    <!-- Support vectors (class A - closest to boundary) -->
                    <circle cx="210" cy="90" r="14" fill="#3b82f6" stroke="#1d4ed8" stroke-width="3">
                        <animate attributeName="r" values="14;17;14" dur="2s" repeatCount="indefinite"/>
                    </circle>
                    <circle cx="210" cy="250" r="14" fill="#3b82f6" stroke="#1d4ed8" stroke-width="3">
                        <animate attributeName="r" values="14;17;14" dur="2s" begin="0.5s" repeatCount="indefinite"/>
                    </circle>

                    <!-- Class B points (orange circles - right side) -->
                    <circle cx="400" cy="50" r="14" fill="#f97316" opacity="0.8"/>
                    <circle cx="370" cy="130" r="14" fill="#f97316" opacity="0.8"/>
                    <circle cx="420" cy="200" r="14" fill="#f97316" opacity="0.8"/>
                    <circle cx="380" cy="270" r="14" fill="#f97316" opacity="0.8"/>
                    <circle cx="440" cy="300" r="14" fill="#f97316" opacity="0.8"/>
                    <circle cx="450" cy="140" r="14" fill="#f97316" opacity="0.8"/>
                    <circle cx="350" cy="60" r="14" fill="#f97316" opacity="0.8"/>

                    <!-- Support vectors (class B) -->
                    <circle cx="290" cy="170" r="14" fill="#f97316" stroke="#c2410c" stroke-width="3">
                        <animate attributeName="r" values="14;17;14" dur="2s" begin="1s" repeatCount="indefinite"/>
                    </circle>

                    <!-- Labels -->
                    <text x="95" y="25" text-anchor="middle" fill="#1d4ed8" font-size="13" font-weight="800" font-family="Nunito">Class A (Blue)</text>
                    <text x="400" y="25" text-anchor="middle" fill="#c2410c" font-size="13" font-weight="800" font-family="Nunito">Class B (Orange)</text>
                    <text x="250" y="10" text-anchor="middle" fill="#9333ea" font-size="11" font-weight="700" font-family="Nunito">HYPERPLANE</text>

                    <!-- SV labels -->
                    <text x="210" y="78" text-anchor="middle" fill="#1e40af" font-size="8" font-weight="700" font-family="Nunito">Support Vector</text>
                    <text x="290" y="158" text-anchor="end" fill="#9a3412" font-size="8" font-weight="700" font-family="Nunito">Support Vector</text>
                </svg>
                <p style="color: #7e22ce; font-size: 0.95em; margin-top: 10px;">The pulsing dots are <strong>support vectors</strong> - the critical points that define the boundary. The shaded area is the <strong>margin</strong>.</p>
            </div>

            <h3>Why Is It Called "Support Vector Machine"?</h3>
            <ul style="margin-left: 22px; color: #334155; margin-bottom: 20px;">
                <li><strong>Support Vectors</strong> = the data points closest to the boundary (the ones that "support" or define where the boundary goes)</li>
                <li><strong>Vector</strong> = a fancy math word for "a point in space with coordinates"</li>
                <li><strong>Machine</strong> = it's a learning machine (algorithm)</li>
            </ul>

            <div class="key-point">
                <h4>üí° The Key Insight</h4>
                <ul>
                    <li>Out of potentially millions of data points, only a <strong>handful</strong> (the support vectors) actually matter for defining the boundary</li>
                    <li>Moving or removing any non-support-vector point does NOT change the boundary at all</li>
                    <li>This makes SVM <strong>memory efficient</strong> and robust to outliers far from the boundary</li>
                </ul>
            </div>
        </div>

        <!-- ======================================== -->
        <!-- Part 2: How SVM Works (The Math Made Easy) -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-cogs"></i> Part 2: How SVM Works (The Math Made Simple)</h2>

            <p>Don't worry - we'll make this painless! SVM is trying to solve one problem: <strong>"What's the best line (or surface) that separates the two classes?"</strong></p>

            <h3>The Hyperplane</h3>
            <p>A <strong>hyperplane</strong> is just a fancy word for a boundary:</p>
            <ul style="margin-left: 22px; color: #334155; margin-bottom: 20px;">
                <li>In <strong>2D</strong> (two features): the hyperplane is a <strong>line</strong></li>
                <li>In <strong>3D</strong> (three features): the hyperplane is a <strong>flat surface</strong> (like a sheet of paper)</li>
                <li>In <strong>100D</strong> (100 features): it's a 99-dimensional surface (can't visualize, but the math works the same!)</li>
            </ul>

            <div class="analogy-box">
                <h4>üçï Pizza Analogy</h4>
                <p>Imagine a pizza with toppings on one half (pepperoni) and different toppings on the other (mushrooms). The hyperplane is the cut that perfectly divides the pizza in half. SVM finds the cut that keeps the widest "crust border" between pepperoni territory and mushroom territory.</p>
            </div>

            <h3>The Margin</h3>
            <p>The <strong>margin</strong> is the distance between the hyperplane and the nearest data point from either class. SVM wants to <strong>maximize this margin</strong>. A wider margin means better generalization to new, unseen data.</p>

            <div class="eli5-box">
                <h4>üõ£Ô∏è The Highway Analogy</h4>
                <p>Think of the hyperplane as a <strong>highway</strong> between two cities (two classes). The support vectors are the buildings closest to the highway on each side. SVM builds the <strong>widest possible highway</strong> so there's maximum clearance from the buildings on both sides. A wider highway means even if a new building is slightly off, it still clearly belongs to its city!</p>
            </div>

            <h3>The Math (Don't Panic - We'll Go Slow!)</h3>

            <!-- FORMULA 1: Dot Product -->
            <div class="formula-box">
                <h4>üìê Formula 1: The Dot Product (the foundation of EVERYTHING)</h4>
                <p>Before we understand SVM's math, we need one building block: the <strong>dot product</strong>. It tells you how much two vectors "agree" in direction.</p>

                <span class="formula-label">THE FORMULA</span>
                <div class="formula">
                    <span class="blue">A</span> ¬∑ <span class="orange">B</span> = |<span class="blue">A</span>| √ó |<span class="orange">B</span>| √ó cos Œ∏
                </div>

                <div class="layman-row">
                    <span class="var">A ¬∑ B</span>
                    <span class="meaning">The dot product ‚Äî a single number that tells you how "aligned" A and B are</span>
                </div>
                <div class="layman-row">
                    <span class="var">|A|</span>
                    <span class="meaning">The length (magnitude) of vector A ‚Äî how "long" the arrow is</span>
                </div>
                <div class="layman-row">
                    <span class="var">|B|</span>
                    <span class="meaning">The length (magnitude) of vector B</span>
                </div>
                <div class="layman-row">
                    <span class="var">cos Œ∏</span>
                    <span class="meaning">The cosine of the angle between A and B (1 = same direction, 0 = perpendicular, -1 = opposite)</span>
                </div>

                <p>Rearranging, we can find the angle between any two vectors:</p>
                <div class="formula">
                    cos Œ∏ = <span class="frac"><span class="frac-num"><span class="blue">A</span> ¬∑ <span class="orange">B</span></span><span class="frac-den">|<span class="blue">A</span>| √ó |<span class="orange">B</span>|</span></span>
                </div>

                <div class="worked-example">
                    <h5>üßÆ Worked Example: Grocery Shopping Vectors</h5>
                    <p>Two shoppers buy items. Shopper A buys: 3 apples, 1 banana. Shopper B buys: 2 apples, 4 bananas.</p>
                    <p>Their shopping vectors are: <strong>A = (3, 1)</strong> and <strong>B = (2, 4)</strong></p>
                    <div class="calc">
                        A ¬∑ B = (3 √ó 2) + (1 √ó 4) = 6 + 4 = <strong>10</strong><br>
                        |A| = ‚àö(3¬≤ + 1¬≤) = ‚àö(9 + 1) = ‚àö10 ‚âà <strong>3.16</strong><br>
                        |B| = ‚àö(2¬≤ + 4¬≤) = ‚àö(4 + 16) = ‚àö20 ‚âà <strong>4.47</strong><br>
                        cos Œ∏ = 10 / (3.16 √ó 4.47) = 10 / 14.14 ‚âà <strong>0.707</strong><br>
                        Œ∏ = arccos(0.707) ‚âà <strong>45¬∞</strong>
                    </div>
                    <p>Their shopping patterns are at a 45¬∞ angle ‚Äî somewhat similar but not identical! If cos Œ∏ = 1, they'd buy the exact same ratio of items.</p>
                </div>
            </div>

            <!-- FORMULA 2: Hyperplane Equation -->
            <div class="formula-box">
                <h4>üìê Formula 2: The Hyperplane Equation</h4>
                <p>The hyperplane is the boundary that SVM draws. In math, it's:</p>

                <span class="formula-label">THE FORMULA</span>
                <div class="formula">
                    <span class="highlight">w</span> ¬∑ <span class="blue">x</span> + <span class="orange">b</span> = 0
                    <span class="small-note">This defines all points ON the boundary line</span>
                </div>

                <div class="layman-row">
                    <span class="var">w</span>
                    <span class="meaning"><strong>Weight vector</strong> ‚Äî determines the DIRECTION / tilt of the boundary. Think of it as which way the fence faces.</span>
                </div>
                <div class="layman-row">
                    <span class="var">x</span>
                    <span class="meaning"><strong>Data point</strong> ‚Äî a specific point in your dataset (e.g., a customer with features: age=25, income=50K)</span>
                </div>
                <div class="layman-row">
                    <span class="var">b</span>
                    <span class="meaning"><strong>Bias</strong> ‚Äî shifts the boundary left/right (or up/down). Like sliding the fence sideways without changing its angle.</span>
                </div>

                <p>For <strong>classification</strong>, we check which side a new point falls on:</p>
                <div class="formula">
                    If <span class="highlight">w</span> ¬∑ <span class="blue">x</span> + <span class="orange">b</span> &gt; 0 ‚Üí <span class="green">Class +1</span> (one side)<br>
                    If <span class="highlight">w</span> ¬∑ <span class="blue">x</span> + <span class="orange">b</span> &lt; 0 ‚Üí <span style="color:#dc2626;">Class -1</span> (other side)
                </div>

                <div class="worked-example">
                    <h5>üßÆ Worked Example: Classifying Fruits</h5>
                    <p>We want to classify fruits as Apples (+1) or Oranges (-1) using two features: weight (x‚ÇÅ) and color_redness (x‚ÇÇ).</p>
                    <p>Say SVM found: <strong>w = (0.6, 0.8)</strong> and <strong>b = -5</strong></p>
                    <p>New fruit has weight = 7, redness = 3:</p>
                    <div class="calc">
                        w ¬∑ x + b = (0.6 √ó 7) + (0.8 √ó 3) + (-5)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = 4.2 + 2.4 - 5<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = <strong>1.6</strong> &gt; 0 ‚Üí It's an <strong>Apple! ‚úÖ</strong>
                    </div>
                    <p>Another fruit: weight = 4, redness = 2:</p>
                    <div class="calc">
                        w ¬∑ x + b = (0.6 √ó 4) + (0.8 √ó 2) + (-5)<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = 2.4 + 1.6 - 5<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; = <strong>-1.0</strong> &lt; 0 ‚Üí It's an <strong>Orange! üçä</strong>
                    </div>
                </div>
            </div>

            <!-- FORMULA 3: Margin Width -->
            <div class="formula-box">
                <h4>üìê Formula 3: The Margin Width</h4>
                <p>The margin is the gap between the two classes. SVM wants to <strong>maximize</strong> this. Here's how it's calculated:</p>

                <p>The support vectors on the positive side satisfy <strong>w ¬∑ x‚Å∫ + b = +1</strong>, and on the negative side: <strong>w ¬∑ x‚Åª + b = -1</strong>.</p>

                <p>Using the dot product and cosine, the margin width is:</p>

                <span class="formula-label">MARGIN WIDTH</span>
                <div class="formula">
                    cos Œ∏ = <span class="frac"><span class="frac-num"><span class="highlight">w</span> ¬∑ (<span class="blue">x‚Å∫</span> - <span style="color:#dc2626;">x‚Åª</span>)</span><span class="frac-den">|<span class="highlight">w</span>| √ó |<span class="blue">x‚Å∫</span> - <span style="color:#dc2626;">x‚Åª</span>|</span></span>
                </div>

                <p>The actual distance (width of the margin road) projected onto the weight vector's direction simplifies beautifully to:</p>

                <div class="formula">
                    Margin = <span class="frac"><span class="frac-num"><span class="green">2</span></span><span class="frac-den">|<span class="highlight">w</span>|</span></span>
                    <span class="small-note">The margin width is simply 2 divided by the length of the weight vector!</span>
                </div>

                <div class="layman-row">
                    <span class="var">|w|</span>
                    <span class="meaning">The length of the weight vector: |w| = ‚àö(w‚ÇÅ¬≤ + w‚ÇÇ¬≤ + ... + w‚Çô¬≤)</span>
                </div>
                <div class="layman-row">
                    <span class="var">2/|w|</span>
                    <span class="meaning">The total width of the "road" between the two classes. Bigger = better!</span>
                </div>

                <p><strong>So maximizing the margin = minimizing |w|!</strong> That's why SVM's optimization objective is to find the smallest possible |w|.</p>

                <div class="worked-example">
                    <h5>üßÆ Worked Example: How Wide is the Road?</h5>
                    <p>Say SVM found weights <strong>w = (3, 4)</strong>.</p>
                    <div class="calc">
                        |w| = ‚àö(3¬≤ + 4¬≤) = ‚àö(9 + 16) = ‚àö25 = <strong>5</strong><br>
                        Margin = 2 / |w| = 2 / 5 = <strong>0.4</strong>
                    </div>
                    <p>Now say another SVM found weights <strong>w = (0.6, 0.8)</strong>.</p>
                    <div class="calc">
                        |w| = ‚àö(0.6¬≤ + 0.8¬≤) = ‚àö(0.36 + 0.64) = ‚àö1 = <strong>1</strong><br>
                        Margin = 2 / |w| = 2 / 1 = <strong>2.0</strong>
                    </div>
                    <p>The second SVM has a margin of 2.0 vs 0.4 ‚Äî <strong>5x wider road!</strong> SVM would prefer the second one because wider margin = better generalization.</p>
                </div>
            </div>

            <!-- FORMULA 4: The Optimization Objective -->
            <div class="formula-box">
                <h4>üìê Formula 4: The SVM Optimization Objective</h4>
                <p>Putting it all together, SVM solves this optimization problem:</p>

                <span class="formula-label">HARD MARGIN (perfect separation)</span>
                <div class="formula">
                    Minimize: <span class="frac"><span class="frac-num">1</span><span class="frac-den">2</span></span> |<span class="highlight">w</span>|¬≤
                    <span class="small-note">Subject to: y·µ¢ (w ¬∑ x·µ¢ + b) ‚â• 1 for ALL data points</span>
                </div>

                <div class="layman-row">
                    <span class="var">¬Ω|w|¬≤</span>
                    <span class="meaning">We minimize the squared length of w (which maximizes the margin). The ¬Ω is just for cleaner math when taking derivatives.</span>
                </div>
                <div class="layman-row">
                    <span class="var">y·µ¢</span>
                    <span class="meaning">The true label of point i: either +1 or -1</span>
                </div>
                <div class="layman-row">
                    <span class="var">y·µ¢(w¬∑x·µ¢+b) ‚â• 1</span>
                    <span class="meaning">Every point must be on the CORRECT side of the boundary AND at least as far as the margin line. If the class is +1, then w¬∑x+b must be ‚â• +1. If class is -1, then w¬∑x+b must be ‚â§ -1.</span>
                </div>

                <div class="analogy-box" style="border-color: #eab308;">
                    <h4>üèóÔ∏è Building a Highway (Layman Version)</h4>
                    <p>The city wants to build a highway. The rules: (1) Make the highway as <strong>wide</strong> as possible (minimize |w|). (2) <strong>No building</strong> can be inside the highway lanes (all y·µ¢(w¬∑x·µ¢+b) ‚â• 1). The city planner (SVM algorithm) finds the widest road that doesn't demolish any building.</p>
                </div>
            </div>

            <p>SVM finds the values of <strong>w</strong> and <strong>b</strong> that maximize the margin while correctly classifying all training points (or allowing some slack for noisy data).</p>

            <h3>Step-by-Step: How SVM Finds the Best Boundary</h3>
            <div class="step-card">
                <div class="step-num">1</div>
                <div><h5>Start with labeled data</h5><p>Each data point has features (X) and a class label (+1 or -1).</p></div>
            </div>
            <div class="step-card">
                <div class="step-num">2</div>
                <div><h5>Find all possible separating hyperplanes</h5><p>There are infinitely many lines that could separate the classes. SVM tests them all (mathematically, via optimization).</p></div>
            </div>
            <div class="step-card">
                <div class="step-num">3</div>
                <div><h5>Pick the one with the MAXIMUM margin</h5><p>The hyperplane with the widest gap to the nearest points on both sides wins. This is found by solving a convex optimization problem (quadratic programming).</p></div>
            </div>
            <div class="step-card">
                <div class="step-num">4</div>
                <div><h5>Identify the support vectors</h5><p>The points that sit exactly on the margin boundary are the support vectors. Only these points influence the final model.</p></div>
            </div>
        </div>

        <!-- ======================================== -->
        <!-- Part 3: Hard Margin vs Soft Margin       -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-balance-scale"></i> Part 3: Hard Margin vs. Soft Margin (The C Parameter)</h2>

            <p>What happens when the data ISN'T perfectly separable? Like when one cat accidentally wandered into the dog side of the playground?</p>

            <h3>Hard Margin SVM</h3>
            <p><strong>Hard margin</strong> means: "I demand PERFECT separation. Not a single point can be on the wrong side!" This only works when data is perfectly linearly separable (rare in real life!).</p>

            <div class="warning-box">
                <h4>‚ö†Ô∏è Problem with Hard Margin</h4>
                <p>Real-world data is messy. There's almost always overlap or noise. Hard margin SVM will either fail completely (no solution exists) or overfit to one weird outlier, creating a terrible boundary. That's why we almost NEVER use hard margin in practice.</p>
            </div>

            <h3>Soft Margin SVM (The Practical One)</h3>
            <p><strong>Soft margin</strong> means: "I'll try to separate perfectly, but I'll <em>tolerate some misclassifications</em> if it gives me a wider, more robust margin." Each misclassified or margin-violating point gets a <strong>penalty</strong>.</p>

            <div class="analogy-box">
                <h4>üè´ The Classroom Analogy</h4>
                <p><strong>Hard margin teacher:</strong> "If even ONE student is sitting on the wrong side of the classroom, I REFUSE to draw the dividing line!" (Impractical - what if a student fell?)</p>
                <p><strong>Soft margin teacher:</strong> "I'll draw the best line I can. If 2 students are slightly on the wrong side, I'll allow it as long as the overall separation is good. Those 2 get a small penalty (detention!), but the line still works great for the other 98 students."</p>
            </div>

            <h3>The C Parameter (Regularization Strength)</h3>
            <p>The <strong>C parameter</strong> controls how much we penalize misclassifications:</p>

            <table class="data-table">
                <thead>
                    <tr><th>C Value</th><th>What Happens</th><th>Analogy</th><th>Risk</th></tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong style="color:#9333ea;">Large C</strong> (e.g., 1000)</td>
                        <td>Heavy penalty for errors. Tries very hard to classify every point correctly. Narrow margin.</td>
                        <td>Strict teacher: "Zero tolerance for mistakes!"</td>
                        <td>Overfitting</td>
                    </tr>
                    <tr>
                        <td><strong style="color:#9333ea;">Small C</strong> (e.g., 0.01)</td>
                        <td>Light penalty for errors. Allows more misclassifications. Wider margin.</td>
                        <td>Chill teacher: "A few mistakes are fine, as long as the big picture works."</td>
                        <td>Underfitting</td>
                    </tr>
                    <tr>
                        <td><strong style="color:#9333ea;">C = 1</strong> (default)</td>
                        <td>Balanced. Usually a good starting point.</td>
                        <td>Reasonable teacher: fair but firm.</td>
                        <td>Good default</td>
                    </tr>
                </tbody>
            </table>

            <!-- FORMULA 5: Soft Margin Objective -->
            <div class="formula-box">
                <h4>üìê Formula 5: The Soft Margin Objective (with C)</h4>
                <p>When data isn't perfectly separable, we add <strong>slack variables</strong> (Œæ) that allow some points to violate the margin:</p>

                <span class="formula-label">SOFT MARGIN</span>
                <div class="formula">
                    Minimize: <span class="frac"><span class="frac-num">1</span><span class="frac-den">2</span></span> |<span class="highlight">w</span>|¬≤ + <span class="orange">C</span> √ó Œ£ <span class="blue">Œæ·µ¢</span>
                    <span class="small-note">Subject to: y·µ¢(w ¬∑ x·µ¢ + b) ‚â• 1 - Œæ·µ¢, and Œæ·µ¢ ‚â• 0</span>
                </div>

                <div class="layman-row">
                    <span class="var">Œæ·µ¢ (xi)</span>
                    <span class="meaning"><strong>Slack variable</strong> ‚Äî how much point i is "allowed to cheat." If Œæ·µ¢ = 0, the point is correctly classified and outside the margin. If 0 &lt; Œæ·µ¢ &lt; 1, it's inside the margin but on the correct side. If Œæ·µ¢ &gt; 1, it's misclassified!</span>
                </div>
                <div class="layman-row">
                    <span class="var">C √ó Œ£Œæ·µ¢</span>
                    <span class="meaning"><strong>Total penalty</strong> ‚Äî C controls how harsh we are. Big C = expensive to cheat = narrow margin. Small C = cheap to cheat = wide margin.</span>
                </div>

                <div class="worked-example">
                    <h5>üßÆ Worked Example: The Parking Ticket Analogy</h5>
                    <p>Think of C as the fine for parking in a no-parking zone (the margin).</p>
                    <div class="calc">
                        C = 1000 (Dubai parking fine!) ‚Üí Drivers are terrified.<br>
                        &nbsp;&nbsp; No one parks there. Margin stays clear but very narrow.<br><br>
                        C = 0.01 (barely any fine) ‚Üí Drivers park wherever they want.<br>
                        &nbsp;&nbsp; The margin is wide but lots of "violations" (misclassifications).
                    </div>
                    <p>Say we have 3 points that violate: Œæ‚ÇÅ = 0.3, Œæ‚ÇÇ = 0.5, Œæ‚ÇÉ = 1.2</p>
                    <div class="calc">
                        With C = 100: Penalty = 100 √ó (0.3 + 0.5 + 1.2) = 100 √ó 2.0 = <strong>200</strong> (ouch!)<br>
                        With C = 0.1: Penalty = 0.1 √ó (0.3 + 0.5 + 1.2) = 0.1 √ó 2.0 = <strong>0.2</strong> (meh)
                    </div>
                    <p>With high C, those 3 violations are very costly, so SVM works harder to avoid them. With low C, SVM barely cares and focuses on a wider margin instead.</p>
                </div>
            </div>

            <div class="key-point">
                <h4>üí° How to Choose C?</h4>
                <ul>
                    <li>Use <strong>cross-validation</strong> to try different C values (e.g., 0.001, 0.01, 0.1, 1, 10, 100, 1000)</li>
                    <li>Pick the C that gives the best validation accuracy</li>
                    <li>Scikit-learn makes this easy with <code>GridSearchCV</code></li>
                </ul>
            </div>
        </div>

        <!-- ======================================== -->
        <!-- Part 4: The Kernel Trick                 -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-magic"></i> Part 4: The Kernel Trick (SVM's Secret Weapon!)</h2>

            <p>What if the data <strong>can't be separated by a straight line at all</strong>? Like if the blue points form a circle surrounded by orange points? No straight line can separate them!</p>

            <div class="svm-visual">
                <h4>The Problem: Non-Linearly Separable Data</h4>
                <svg viewBox="0 0 500 250" xmlns="http://www.w3.org/2000/svg">
                    <!-- Inner circle points (blue) -->
                    <circle cx="250" cy="125" r="12" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="230" cy="100" r="12" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="270" cy="100" r="12" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="240" cy="145" r="12" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="260" cy="145" r="12" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="250" cy="90" r="12" fill="#3b82f6" opacity="0.8"/>
                    <circle cx="235" cy="125" r="12" fill="#3b82f6" opacity="0.8"/>

                    <!-- Outer ring points (orange) -->
                    <circle cx="150" cy="60" r="12" fill="#f97316" opacity="0.8"/>
                    <circle cx="350" cy="60" r="12" fill="#f97316" opacity="0.8"/>
                    <circle cx="100" cy="125" r="12" fill="#f97316" opacity="0.8"/>
                    <circle cx="400" cy="125" r="12" fill="#f97316" opacity="0.8"/>
                    <circle cx="150" cy="190" r="12" fill="#f97316" opacity="0.8"/>
                    <circle cx="350" cy="190" r="12" fill="#f97316" opacity="0.8"/>
                    <circle cx="200" cy="40" r="12" fill="#f97316" opacity="0.8"/>
                    <circle cx="300" cy="40" r="12" fill="#f97316" opacity="0.8"/>
                    <circle cx="200" cy="210" r="12" fill="#f97316" opacity="0.8"/>
                    <circle cx="300" cy="210" r="12" fill="#f97316" opacity="0.8"/>
                    <circle cx="130" cy="80" r="12" fill="#f97316" opacity="0.8"/>
                    <circle cx="370" cy="80" r="12" fill="#f97316" opacity="0.8"/>

                    <text x="250" y="240" text-anchor="middle" fill="#991b1b" font-size="12" font-weight="700" font-family="Nunito">No straight line can separate these! We need the kernel trick.</text>
                </svg>
            </div>

            <h3>The Magic: Transform to Higher Dimensions!</h3>

            <div class="eli5-box">
                <h4>üé™ The Circus Trick Analogy</h4>
                <p>Imagine blue coins and orange coins scattered on a table. The blue coins are in the center, orange coins surround them. No straight ruler can separate them on the <strong>flat table</strong> (2D).</p>
                <p>Now imagine you <strong>SLAM the table</strong> from below! üí• The coins fly up into the air. The blue coins (lighter) fly higher, the orange ones (heavier) stay lower. NOW, in 3D space, you CAN draw a flat sheet between them!</p>
                <p>That "slamming" is the <strong>kernel trick</strong>. It projects data into a higher dimension where a linear boundary WORKS. The brilliant part? SVM does this <strong>without actually computing</strong> the higher-dimensional coordinates (saving massive computation). It uses a mathematical shortcut called the <strong>kernel function</strong>.</p>
            </div>

            <h3>Types of Kernels</h3>

            <table class="data-table">
                <thead>
                    <tr><th>Kernel</th><th>When to Use</th><th>What It Does</th><th>Speed</th></tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong style="color:#9333ea;">Linear</strong><br><code>kernel='linear'</code></td>
                        <td>Data is (mostly) linearly separable, or you have LOTS of features (text, genomics)</td>
                        <td>No transformation. Just finds the best straight line/plane.</td>
                        <td>Fastest</td>
                    </tr>
                    <tr>
                        <td><strong style="color:#9333ea;">RBF / Gaussian</strong><br><code>kernel='rbf'</code></td>
                        <td>Most common default. Works well when you're not sure about the data shape.</td>
                        <td>Maps to infinite dimensions! Can handle very complex, curvy boundaries.</td>
                        <td>Medium</td>
                    </tr>
                    <tr>
                        <td><strong style="color:#9333ea;">Polynomial</strong><br><code>kernel='poly'</code></td>
                        <td>When relationships are polynomial (e.g., x1*x2 or x1^2 matters)</td>
                        <td>Maps to a higher (finite) dimensional space. Controlled by degree parameter.</td>
                        <td>Slower</td>
                    </tr>
                    <tr>
                        <td><strong style="color:#9333ea;">Sigmoid</strong><br><code>kernel='sigmoid'</code></td>
                        <td>Rarely used. Similar to a neural network with one hidden layer.</td>
                        <td>Uses tanh function as the kernel. Mostly for specific research use cases.</td>
                        <td>Medium</td>
                    </tr>
                </tbody>
            </table>

            <!-- FORMULA 6: Kernel Functions -->
            <div class="formula-box">
                <h4>üìê Formula 6: The Kernel Functions</h4>
                <p>Each kernel is a function K(x·µ¢, x‚±º) that computes the similarity between two data points ‚Äî but in a HIGHER dimensional space, without actually going there!</p>

                <span class="formula-label">LINEAR KERNEL</span>
                <div class="formula">
                    K(<span class="blue">x·µ¢</span>, <span class="orange">x‚±º</span>) = <span class="blue">x·µ¢</span> ¬∑ <span class="orange">x‚±º</span>
                    <span class="small-note">Just the plain dot product. No transformation at all.</span>
                </div>

                <span class="formula-label">POLYNOMIAL KERNEL</span>
                <div class="formula">
                    K(<span class="blue">x·µ¢</span>, <span class="orange">x‚±º</span>) = (<span class="blue">x·µ¢</span> ¬∑ <span class="orange">x‚±º</span> + r)<sup style="font-size:0.7em;">d</sup>
                    <span class="small-note">r = constant (default 0), d = degree (default 3). Creates polynomial features automatically!</span>
                </div>

                <span class="formula-label">RBF (GAUSSIAN) KERNEL ‚Äî THE STAR ‚≠ê</span>
                <div class="formula">
                    K(<span class="blue">x·µ¢</span>, <span class="orange">x‚±º</span>) = e<sup style="font-size:0.7em;">-<span class="highlight">Œ≥</span> √ó |<span class="blue">x·µ¢</span> - <span class="orange">x‚±º</span>|¬≤</sup>
                    <span class="small-note">Œ≥ (gamma) controls the "reach" of each point. |x·µ¢ - x‚±º|¬≤ is the squared distance between the two points.</span>
                </div>

                <div class="layman-row">
                    <span class="var">e‚ÅªÀ£</span>
                    <span class="meaning">The exponential function. As x gets bigger, e‚ÅªÀ£ gets SMALLER (approaches 0). So distant points ‚Üí kernel value near 0 (very different). Close points ‚Üí kernel value near 1 (very similar).</span>
                </div>
                <div class="layman-row">
                    <span class="var">Œ≥ (gamma)</span>
                    <span class="meaning">Controls the "radius of influence." High gamma = only very close points matter (tight circles). Low gamma = far-away points still matter (wide circles).</span>
                </div>

                <div class="worked-example">
                    <h5>üßÆ Worked Example: RBF Kernel with Two Points</h5>
                    <p>Point A = (1, 2), Point B = (3, 4), gamma = 0.5</p>
                    <div class="calc">
                        |A - B|¬≤ = (1-3)¬≤ + (2-4)¬≤ = 4 + 4 = <strong>8</strong><br>
                        K(A, B) = e<sup>-0.5 √ó 8</sup> = e<sup>-4</sup> ‚âà <strong>0.018</strong> (very different!)<br><br>

                        Now try Point C = (1, 2) and Point D = (1.5, 2.5):<br>
                        |C - D|¬≤ = (0.5)¬≤ + (0.5)¬≤ = 0.25 + 0.25 = <strong>0.5</strong><br>
                        K(C, D) = e<sup>-0.5 √ó 0.5</sup> = e<sup>-0.25</sup> ‚âà <strong>0.78</strong> (very similar!)
                    </div>
                    <p>Points close together ‚Üí high kernel value (similar). Points far apart ‚Üí low kernel value (different). The RBF kernel is basically asking: <strong>"How close are you to your neighbor?"</strong></p>
                </div>
            </div>

            <h3>The Gamma Parameter (for RBF Kernel)</h3>
            <p>The <strong>gamma</strong> parameter controls how far the influence of a single training example reaches:</p>

            <div class="comparison-grid">
                <div class="comparison-card">
                    <h5>High Gamma</h5>
                    <p>Each point has very <strong>local</strong> influence. The boundary becomes very wiggly, hugging each point closely. Risk: <strong>overfitting</strong>.</p>
                    <p style="font-size: 0.85em; color: #9333ea; margin-top: 8px;">Like looking at the world through a magnifying glass - you see every tiny detail but miss the big picture.</p>
                </div>
                <div class="comparison-card">
                    <h5>Low Gamma</h5>
                    <p>Each point has very <strong>wide</strong> influence. The boundary is smoother and more general. Risk: <strong>underfitting</strong>.</p>
                    <p style="font-size: 0.85em; color: #9333ea; margin-top: 8px;">Like looking at the world from an airplane - you see the big picture but miss individual details.</p>
                </div>
            </div>

            <div class="key-point">
                <h4>üí° C and Gamma Together</h4>
                <ul>
                    <li><strong>High C + High Gamma</strong> = complex boundary, tight fit ‚Üí likely overfitting</li>
                    <li><strong>Low C + Low Gamma</strong> = simple boundary, loose fit ‚Üí likely underfitting</li>
                    <li>Find the sweet spot with <strong>GridSearchCV</strong>, trying combinations like C=[0.1, 1, 10, 100] and gamma=[0.001, 0.01, 0.1, 1]</li>
                </ul>
            </div>

            <!-- FORMULA 7: Hinge Loss -->
            <div class="formula-box">
                <h4>üìê Formula 7: The Hinge Loss (How SVM Penalizes Mistakes)</h4>
                <p>Internally, SVM uses a special loss function called <strong>Hinge Loss</strong>. Unlike other loss functions, it's happy as long as you're on the right side AND far enough away:</p>

                <span class="formula-label">HINGE LOSS</span>
                <div class="formula">
                    Loss = max(0, 1 - y·µ¢ √ó (<span class="highlight">w</span> ¬∑ <span class="blue">x·µ¢</span> + <span class="orange">b</span>))
                    <span class="small-note">y·µ¢ is the true label (+1 or -1). The loss is 0 when the point is correctly classified AND outside the margin.</span>
                </div>

                <div class="worked-example">
                    <h5>üßÆ Worked Example: When Does the Loss Kick In?</h5>
                    <p>Say we have a positive point (y = +1) and our SVM computes w¬∑x+b for it:</p>
                    <div class="calc">
                        <strong>Case 1:</strong> w¬∑x+b = 2.5 (correct side, far from margin)<br>
                        Loss = max(0, 1 - 1√ó2.5) = max(0, -1.5) = <strong>0</strong> ‚úÖ No penalty!<br><br>

                        <strong>Case 2:</strong> w¬∑x+b = 0.7 (correct side, but INSIDE the margin)<br>
                        Loss = max(0, 1 - 1√ó0.7) = max(0, 0.3) = <strong>0.3</strong> ‚ö†Ô∏è Small penalty<br><br>

                        <strong>Case 3:</strong> w¬∑x+b = -0.5 (WRONG side! misclassified)<br>
                        Loss = max(0, 1 - 1√ó(-0.5)) = max(0, 1.5) = <strong>1.5</strong> üö® Big penalty!
                    </div>
                    <p>Only Case 1 (correctly classified AND outside the margin) gets zero loss. That's why SVM cares about both correctness AND margin distance!</p>
                </div>

                <div class="analogy-box" style="border-color: #eab308;">
                    <h4>üèÉ The Race Lane Analogy</h4>
                    <p>Imagine a running race. The lane marker is the hyperplane, and the "safe zone" is 1 meter beyond the lane. If you're in your lane AND past the safe zone ‚Üí no penalty. If you drift INTO the safe zone but still in your lane ‚Üí small penalty. If you cross into the other runner's lane ‚Üí BIG penalty. That's hinge loss!</p>
                </div>
            </div>
        </div>

        <!-- ======================================== -->
        <!-- Part 5: SVM for Regression (SVR)         -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-chart-line"></i> Part 5: SVM for Regression (SVR)</h2>

            <p>SVM isn't just for classification! <strong>Support Vector Regression (SVR)</strong> flips the idea: instead of finding the widest margin between classes, it finds a tube (called the <strong>epsilon-tube</strong>) around the prediction line, and tries to fit as many points INSIDE the tube as possible.</p>

            <div class="analogy-box">
                <h4>üöá The Subway Tunnel Analogy</h4>
                <p>Imagine drawing a line through your data (the regression line). Now inflate it into a <strong>tube/tunnel</strong> of width epsilon (Œµ). Points INSIDE the tube? No penalty. Points OUTSIDE the tube? They get penalized (they're errors). SVR finds the line and tube that contains the most points with the flattest (simplest) line possible.</p>
            </div>

            <div class="code-block">
<pre><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVR
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split
<span class="keyword">import</span> numpy <span class="keyword">as</span> np

<span class="comment"># Create sample data</span>
np.random.<span class="function">seed</span>(<span class="number">42</span>)
X = np.<span class="function">sort</span>(<span class="number">5</span> * np.random.<span class="function">rand</span>(<span class="number">100</span>, <span class="number">1</span>), axis=<span class="number">0</span>)
y = np.<span class="function">sin</span>(X).<span class="function">ravel</span>() + np.random.<span class="function">normal</span>(<span class="number">0</span>, <span class="number">0.1</span>, X.shape[<span class="number">0</span>])

<span class="comment"># Scale features (ALWAYS scale for SVM!)</span>
scaler = <span class="function">StandardScaler</span>()
X_scaled = scaler.<span class="function">fit_transform</span>(X)

<span class="comment"># Create SVR with RBF kernel</span>
svr = <span class="function">SVR</span>(kernel=<span class="string">'rbf'</span>, C=<span class="number">100</span>, gamma=<span class="number">0.1</span>, epsilon=<span class="number">0.1</span>)
svr.<span class="function">fit</span>(X_scaled, y)

<span class="comment"># Predict</span>
y_pred = svr.<span class="function">predict</span>(X_scaled)
<span class="function">print</span>(<span class="string">f"R¬≤ Score: {svr.score(X_scaled, y):.4f}"</span>)
<span class="function">print</span>(<span class="string">f"Number of support vectors: {len(svr.support_)}"</span>)</pre>
            </div>
        </div>

        <!-- ======================================== -->
        <!-- Part 6: SVM for Multi-Class              -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-layer-group"></i> Part 6: Multi-Class Classification</h2>

            <p>SVM is natively a <strong>binary classifier</strong> (two classes only). But what if you have 3, 5, or 10 classes? Two strategies:</p>

            <div class="comparison-grid">
                <div class="comparison-card">
                    <h5>One-vs-Rest (OvR)</h5>
                    <p>Train K separate SVMs (one for each class). Each SVM asks: "Is this point Class A or Not A?" For 10 classes, train 10 SVMs. Assign the class whose SVM gives the highest confidence.</p>
                    <p style="font-size: 0.85em; color: #9333ea; margin-top: 8px;">Faster, fewer models. Used by <code>LinearSVC</code> by default.</p>
                </div>
                <div class="comparison-card">
                    <h5>One-vs-One (OvO)</h5>
                    <p>Train an SVM for every PAIR of classes. For 10 classes, that's 45 SVMs! Each one votes. The class with the most votes wins.</p>
                    <p style="font-size: 0.85em; color: #9333ea; margin-top: 8px;">More models but each trains on less data. Used by <code>SVC</code> by default.</p>
                </div>
            </div>

            <div class="key-point">
                <h4>üí° Don't Worry About This!</h4>
                <ul>
                    <li>Scikit-learn handles multi-class <strong>automatically</strong>! Just pass your multi-class labels and it picks OvR or OvO internally.</li>
                    <li><code>SVC()</code> uses One-vs-One by default</li>
                    <li><code>LinearSVC()</code> uses One-vs-Rest by default</li>
                </ul>
            </div>
        </div>

        <!-- ======================================== -->
        <!-- Part 7: Feature Scaling                  -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-ruler-combined"></i> Part 7: Feature Scaling (CRITICAL for SVM!)</h2>

            <p>SVM is <strong>extremely sensitive</strong> to feature scales. If one feature ranges from 0-1 and another from 0-1,000,000, the large feature will dominate the distance calculations and the model will be terrible.</p>

            <div class="warning-box">
                <h4>‚ö†Ô∏è ALWAYS Scale Before SVM!</h4>
                <p>This is not optional. SVM REQUIRES scaled features to work properly. Use StandardScaler (zero mean, unit variance) or MinMaxScaler (0 to 1). This is the #1 mistake beginners make with SVM!</p>
            </div>

            <div class="analogy-box">
                <h4>‚öñÔ∏è Why Scaling Matters</h4>
                <p>Imagine comparing houses by "number of bedrooms" (1-5) and "price in dollars" (100,000-5,000,000). Without scaling, the price dominates everything because 5,000,000 >> 5. The bedroom count is essentially ignored! Scaling puts both features on equal footing so SVM can consider them fairly.</p>
            </div>

            <div class="code-block">
<pre><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler
<span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline
<span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC

<span class="comment"># BEST PRACTICE: use a Pipeline so scaling is automatic</span>
svm_pipeline = <span class="function">Pipeline</span>([
    (<span class="string">'scaler'</span>, <span class="function">StandardScaler</span>()),
    (<span class="string">'svm'</span>, <span class="function">SVC</span>(kernel=<span class="string">'rbf'</span>, C=<span class="number">1.0</span>, gamma=<span class="string">'scale'</span>))
])

<span class="comment"># Now just fit and predict - scaling happens automatically!</span>
svm_pipeline.<span class="function">fit</span>(X_train, y_train)
y_pred = svm_pipeline.<span class="function">predict</span>(X_test)</pre>
            </div>
        </div>

        <!-- ======================================== -->
        <!-- Part 8: Complete Python Example          -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-code"></i> Part 8: Complete SVM in Python (Step by Step)</h2>

            <p>Let's build a complete SVM classifier on a real dataset. We'll use the <strong>Breast Cancer Wisconsin</strong> dataset (built into scikit-learn) to classify tumors as malignant or benign.</p>

            <div class="code-block">
<pre><span class="comment"># ============================================</span>
<span class="comment"># COMPLETE SVM CLASSIFICATION EXAMPLE</span>
<span class="comment"># Dataset: Breast Cancer Wisconsin</span>
<span class="comment"># Goal: Classify tumors as malignant or benign</span>
<span class="comment"># ============================================</span>

<span class="keyword">import</span> numpy <span class="keyword">as</span> np
<span class="keyword">import</span> pandas <span class="keyword">as</span> pd
<span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_breast_cancer
<span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler
<span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC
<span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> (classification_report,
    confusion_matrix, accuracy_score, roc_auc_score)
<span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline

<span class="comment"># ‚îÄ‚îÄ Step 1: Load the data ‚îÄ‚îÄ</span>
data = <span class="function">load_breast_cancer</span>()
X = data.data
y = data.target
<span class="function">print</span>(<span class="string">f"Dataset shape: {X.shape}"</span>)
<span class="function">print</span>(<span class="string">f"Classes: {data.target_names}"</span>)
<span class="function">print</span>(<span class="string">f"Features: {data.feature_names[:5]}..."</span>)

<span class="comment"># ‚îÄ‚îÄ Step 2: Split into train/test ‚îÄ‚îÄ</span>
X_train, X_test, y_train, y_test = <span class="function">train_test_split</span>(
    X, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>, stratify=y
)
<span class="function">print</span>(<span class="string">f"\nTrain size: {len(X_train)}, Test size: {len(X_test)}"</span>)

<span class="comment"># ‚îÄ‚îÄ Step 3: Create pipeline (Scale + SVM) ‚îÄ‚îÄ</span>
pipeline = <span class="function">Pipeline</span>([
    (<span class="string">'scaler'</span>, <span class="function">StandardScaler</span>()),
    (<span class="string">'svm'</span>, <span class="function">SVC</span>(probability=<span class="keyword">True</span>))
])

<span class="comment"># ‚îÄ‚îÄ Step 4: Hyperparameter Tuning with GridSearchCV ‚îÄ‚îÄ</span>
param_grid = {
    <span class="string">'svm__C'</span>: [<span class="number">0.1</span>, <span class="number">1</span>, <span class="number">10</span>, <span class="number">100</span>],
    <span class="string">'svm__gamma'</span>: [<span class="number">0.001</span>, <span class="number">0.01</span>, <span class="number">0.1</span>, <span class="number">1</span>],
    <span class="string">'svm__kernel'</span>: [<span class="string">'rbf'</span>, <span class="string">'linear'</span>]
}

grid_search = <span class="function">GridSearchCV</span>(
    pipeline, param_grid,
    cv=<span class="number">5</span>, scoring=<span class="string">'accuracy'</span>,
    n_jobs=-<span class="number">1</span>, verbose=<span class="number">0</span>
)
grid_search.<span class="function">fit</span>(X_train, y_train)

<span class="function">print</span>(<span class="string">f"\nBest Parameters: {grid_search.best_params_}"</span>)
<span class="function">print</span>(<span class="string">f"Best CV Accuracy: {grid_search.best_score_:.4f}"</span>)

<span class="comment"># ‚îÄ‚îÄ Step 5: Evaluate on test set ‚îÄ‚îÄ</span>
best_model = grid_search.best_estimator_
y_pred = best_model.<span class="function">predict</span>(X_test)
y_proba = best_model.<span class="function">predict_proba</span>(X_test)[:, <span class="number">1</span>]

<span class="function">print</span>(<span class="string">f"\nTest Accuracy: {accuracy_score(y_test, y_pred):.4f}"</span>)
<span class="function">print</span>(<span class="string">f"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}"</span>)
<span class="function">print</span>(<span class="string">f"\nClassification Report:"</span>)
<span class="function">print</span>(<span class="function">classification_report</span>(y_test, y_pred,
    target_names=data.target_names))

<span class="comment"># ‚îÄ‚îÄ Step 6: Check support vectors ‚îÄ‚îÄ</span>
svm_model = best_model.named_steps[<span class="string">'svm'</span>]
<span class="function">print</span>(<span class="string">f"Number of support vectors: {svm_model.n_support_}"</span>)
<span class="function">print</span>(<span class="string">f"Total support vectors: {sum(svm_model.n_support_)}"</span>)
<span class="function">print</span>(<span class="string">f"Out of {len(X_train)} training samples"</span>)</pre>
            </div>

            <div class="key-point">
                <h4>üí° Key Things to Notice</h4>
                <ul>
                    <li>We used a <strong>Pipeline</strong> to combine scaling + SVM (best practice!)</li>
                    <li>We used <strong>GridSearchCV</strong> with 5-fold cross-validation to find the best C, gamma, and kernel</li>
                    <li><code>probability=True</code> enables probability estimates (needed for ROC AUC)</li>
                    <li><code>stratify=y</code> in train_test_split ensures balanced class distribution</li>
                    <li>We checked the number of support vectors - typically a small fraction of total training data</li>
                </ul>
            </div>
        </div>

        <!-- ======================================== -->
        <!-- Part 9: LinearSVC vs SVC                 -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-bolt"></i> Part 9: LinearSVC vs. SVC (When Speed Matters)</h2>

            <p>Scikit-learn offers two SVM classes. Knowing when to use which is key:</p>

            <table class="data-table">
                <thead>
                    <tr><th>Feature</th><th>SVC</th><th>LinearSVC</th></tr>
                </thead>
                <tbody>
                    <tr><td><strong>Kernels</strong></td><td>linear, rbf, poly, sigmoid</td><td>Linear only</td></tr>
                    <tr><td><strong>Speed</strong></td><td>Slower (O(n¬≤) to O(n¬≥))</td><td>Much faster (O(n))</td></tr>
                    <tr><td><strong>Large datasets</strong></td><td>Struggles above 10K-50K samples</td><td>Handles 100K+ easily</td></tr>
                    <tr><td><strong>Multi-class</strong></td><td>One-vs-One (default)</td><td>One-vs-Rest (default)</td></tr>
                    <tr><td><strong>Probabilities</strong></td><td>Yes (with probability=True)</td><td>Not directly (use CalibratedClassifierCV)</td></tr>
                    <tr><td><strong>Best for</strong></td><td>Small-medium data with non-linear boundaries</td><td>Large data, text classification, high-dimensional data</td></tr>
                </tbody>
            </table>

            <div class="code-block">
<pre><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC
<span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler

<span class="comment"># For large datasets or text data, use LinearSVC</span>
fast_svm = <span class="function">Pipeline</span>([
    (<span class="string">'scaler'</span>, <span class="function">StandardScaler</span>()),
    (<span class="string">'svm'</span>, <span class="function">LinearSVC</span>(C=<span class="number">1.0</span>, max_iter=<span class="number">10000</span>))
])
fast_svm.<span class="function">fit</span>(X_train, y_train)
<span class="function">print</span>(<span class="string">f"Accuracy: {fast_svm.score(X_test, y_test):.4f}"</span>)</pre>
            </div>
        </div>

        <!-- ======================================== -->
        <!-- Part 10: Pros, Cons & When to Use        -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-balance-scale-right"></i> Part 10: Pros, Cons &amp; When to Use SVM</h2>

            <h3>Advantages</h3>
            <div class="key-point">
                <h4>‚úÖ Why SVM is Great</h4>
                <ul>
                    <li><strong>Works in high dimensions</strong> - Even when features outnumber samples (text classification, genomics)</li>
                    <li><strong>Memory efficient</strong> - Only stores support vectors, not all training data</li>
                    <li><strong>Versatile kernels</strong> - RBF kernel handles complex non-linear boundaries beautifully</li>
                    <li><strong>Robust to overfitting</strong> in high-dimensional spaces (with proper C tuning)</li>
                    <li><strong>Works well with clear margin</strong> of separation between classes</li>
                    <li><strong>Effective on small-to-medium datasets</strong> - Often outperforms other algorithms</li>
                </ul>
            </div>

            <h3>Disadvantages</h3>
            <div class="warning-box">
                <h4>‚ùå When SVM Struggles</h4>
                <ul style="margin-left: 22px; color: #991b1b;">
                    <li><strong>Slow on large datasets</strong> - Training time is O(n¬≤) to O(n¬≥) for kernel SVM. 100K+ rows? Use LinearSVC or another algorithm.</li>
                    <li><strong>Sensitive to feature scaling</strong> - MUST scale features (easy to forget!)</li>
                    <li><strong>Hard to interpret</strong> - Unlike decision trees, you can't easily explain "why" a prediction was made</li>
                    <li><strong>Noisy data with overlapping classes</strong> - When classes heavily overlap, SVM may not be the best choice</li>
                    <li><strong>Choosing the right kernel and hyperparameters</strong> - Requires experimentation with GridSearchCV</li>
                    <li><strong>No native probability estimates</strong> - Uses Platt scaling which can be slow</li>
                </ul>
            </div>

            <h3>When Should You Use SVM?</h3>
            <table class="data-table">
                <thead>
                    <tr><th>Scenario</th><th>Use SVM?</th><th>Why / Alternative</th></tr>
                </thead>
                <tbody>
                    <tr><td>Text classification (spam detection)</td><td><strong style="color: #16a34a;">YES</strong></td><td>High-dimensional, sparse data. LinearSVC excels here!</td></tr>
                    <tr><td>Image classification (small dataset)</td><td><strong style="color: #16a34a;">YES</strong></td><td>SVM with RBF kernel works great on small image datasets</td></tr>
                    <tr><td>Tabular data with 1M+ rows</td><td><strong style="color: #dc2626;">NO</strong></td><td>Too slow. Use XGBoost, Random Forest, or neural networks</td></tr>
                    <tr><td>Need to explain predictions</td><td><strong style="color: #dc2626;">NO</strong></td><td>SVM is a black box. Use Decision Trees or Logistic Regression</td></tr>
                    <tr><td>Medical diagnosis (small dataset)</td><td><strong style="color: #16a34a;">YES</strong></td><td>SVM is excellent with small, high-dimensional medical data</td></tr>
                    <tr><td>Binary classification baseline</td><td><strong style="color: #16a34a;">YES</strong></td><td>Great baseline to compare against other models</td></tr>
                    <tr><td>Regression with non-linear patterns</td><td><strong style="color: #f59e0b;">MAYBE</strong></td><td>SVR works but XGBoost/Random Forest often better</td></tr>
                </tbody>
            </table>
        </div>

        <!-- ======================================== -->
        <!-- Part 11: SVM vs Other Algorithms         -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-users"></i> Part 11: SVM vs. Other Algorithms</h2>

            <table class="data-table">
                <thead>
                    <tr><th>Algorithm</th><th>Speed</th><th>Interpretability</th><th>Handles Non-Linear</th><th>Large Data</th><th>Best For</th></tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>SVM (RBF)</strong></td>
                        <td>Slow</td><td>Low</td><td>Excellent</td><td>Poor</td>
                        <td>Small-medium data, clear margins</td>
                    </tr>
                    <tr>
                        <td><strong>Logistic Regression</strong></td>
                        <td>Fast</td><td>High</td><td>No (linear only)</td><td>Good</td>
                        <td>Interpretable linear classification</td>
                    </tr>
                    <tr>
                        <td><strong>kNN</strong></td>
                        <td>Fast train, slow predict</td><td>Medium</td><td>Yes</td><td>Poor</td>
                        <td>Simple baseline, local patterns</td>
                    </tr>
                    <tr>
                        <td><strong>Decision Tree</strong></td>
                        <td>Fast</td><td>Very High</td><td>Yes</td><td>Good</td>
                        <td>Explainable models</td>
                    </tr>
                    <tr>
                        <td><strong>Random Forest</strong></td>
                        <td>Medium</td><td>Medium</td><td>Yes</td><td>Good</td>
                        <td>General purpose, robust</td>
                    </tr>
                    <tr>
                        <td><strong>XGBoost</strong></td>
                        <td>Fast</td><td>Medium</td><td>Yes</td><td>Excellent</td>
                        <td>Competitions, tabular data</td>
                    </tr>
                </tbody>
            </table>
        </div>

        <!-- ======================================== -->
        <!-- Part 12: Summary & Cheat Sheet           -->
        <!-- ======================================== -->
        <div class="section">
            <h2><i class="fas fa-trophy"></i> Part 12: Summary &amp; Cheat Sheet</h2>

            <div class="key-point">
                <h4>üìù Everything You Need to Remember</h4>
                <ul>
                    <li><strong>SVM</strong> finds the hyperplane with the maximum margin between classes. Only <strong>support vectors</strong> (nearest points) matter.</li>
                    <li><strong>C parameter</strong>: High C = strict (overfit risk), Low C = relaxed (underfit risk). Tune with GridSearchCV.</li>
                    <li><strong>Kernel trick</strong>: Projects data into higher dimensions so non-linear data becomes linearly separable. Use <strong>RBF</strong> (default) for non-linear, <strong>linear</strong> for large/high-dimensional data.</li>
                    <li><strong>Gamma parameter</strong> (RBF kernel): High = wiggly boundary (overfit), Low = smooth boundary (underfit).</li>
                    <li><strong>ALWAYS SCALE</strong> your features before SVM. Use <code>StandardScaler</code> inside a <code>Pipeline</code>.</li>
                    <li><strong>SVC</strong> for small-medium data with kernels. <strong>LinearSVC</strong> for large data with linear boundaries.</li>
                    <li><strong>SVR</strong> for regression: fits an epsilon-tube around the prediction line.</li>
                    <li><strong>Best use cases</strong>: text classification, medical data, image classification (small datasets), any problem with clear margins and moderate data size.</li>
                    <li><strong>Avoid when</strong>: dataset is very large (100K+ rows), you need interpretability, or classes heavily overlap.</li>
                </ul>
            </div>

            <div class="code-block">
<pre><span class="comment"># ‚îÄ‚îÄ QUICK REFERENCE CHEAT SHEET ‚îÄ‚îÄ</span>

<span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC, LinearSVC, SVR
<span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler
<span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline

<span class="comment"># Classification with RBF kernel (small-medium data)</span>
clf = <span class="function">Pipeline</span>([
    (<span class="string">'scaler'</span>, <span class="function">StandardScaler</span>()),
    (<span class="string">'svm'</span>, <span class="function">SVC</span>(kernel=<span class="string">'rbf'</span>, C=<span class="number">1</span>, gamma=<span class="string">'scale'</span>))
])

<span class="comment"># Fast linear classification (large data, text)</span>
clf_fast = <span class="function">Pipeline</span>([
    (<span class="string">'scaler'</span>, <span class="function">StandardScaler</span>()),
    (<span class="string">'svm'</span>, <span class="function">LinearSVC</span>(C=<span class="number">1</span>, max_iter=<span class="number">10000</span>))
])

<span class="comment"># Regression</span>
reg = <span class="function">Pipeline</span>([
    (<span class="string">'scaler'</span>, <span class="function">StandardScaler</span>()),
    (<span class="string">'svr'</span>, <span class="function">SVR</span>(kernel=<span class="string">'rbf'</span>, C=<span class="number">100</span>, epsilon=<span class="number">0.1</span>))
])</pre>
            </div>

            <p style="margin-top: 20px;">Next, head to <a href="decision-trees.html" style="color: #9333ea; font-weight: 700;">Decision Trees & Random Forests</a> to learn about tree-based models, or go back to <a href="k-nearest-neighbors.html" style="color: #9333ea; font-weight: 700;">kNN</a> to compare approaches.</p>
        </div>

        <div class="nav-buttons">
            <a href="k-nearest-neighbors.html" class="nav-btn prev"><i class="fas fa-arrow-left"></i> Previous: kNN</a>
            <a href="decision-trees.html" class="nav-btn next">Next: Decision Trees <i class="fas fa-arrow-right"></i></a>
        </div>
    </div>

    <button class="back-to-top" id="backToTop"><i class="fas fa-arrow-up"></i></button>
    <script>
        const backToTopButton = document.getElementById('backToTop');
        window.addEventListener('scroll', () => {
            if (window.pageYOffset > 300) backToTopButton.classList.add('show');
            else backToTopButton.classList.remove('show');
        });
        backToTopButton.addEventListener('click', () => window.scrollTo({ top: 0, behavior: 'smooth' }));
    </script>
</body>
</html>
